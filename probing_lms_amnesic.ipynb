{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9IsZIGEwCM"
      },
      "source": [
        "Google Colab stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V7o8UHIEwCO",
        "outputId": "e1d8756d-5960-40bf-ec19-977d505a0850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install dependencies\n",
        "%pip install transformers\n",
        "%pip install pytorch\n",
        "%pip install scikit-learn\n",
        "%pip install conllu\n",
        "%pip install tqdm\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/nlp2-probing-lms/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f23bc515b0574a1ab59893f28bf00da6",
            "f54acfa0adbb476a95b4c5205165f589",
            "cbdb9ce9f3044615986869f7ea2bb4ec",
            "5617402ea18a459e852ea61d9bbea8a3",
            "767ac8b7dbaa492eb0251f10310ed374",
            "b01663fe49b04eaab75d9b8904d91ce0",
            "e88da32b93c04c2c833eee14bd93b475",
            "fb1a60c920f84539bc322f2b4205303b",
            "edcc7b1ab38e479a96dd167137acb75b",
            "68c7510199364886a3c67daacc5f8ebf",
            "482803b02786410cb05131744eb057e4",
            "30db659aef8a443395f2c2809dabc0e2",
            "94439f8e4023486fa8467c2992693c1b",
            "814dc358cad04919a12305e9c5e65fe3",
            "aa2597aa60614a2c9105c699ebc89abd",
            "bc1a110f5a2d45c8b2b416e1aad0b741",
            "646022388cce424d9ffe8057468fea85",
            "4b3cb13b2d9c4f198e8db5f764f9a2b8",
            "c3dabfff875349ccb1a26fb2af68c720",
            "c6a57c969fac4cc99bbd52fde33c7501",
            "c45284ca198543f6b13a580a2c7aba36",
            "0f62a5469f5e476697e7ff0b94fb2707",
            "7e07bbd681b44941858dd0e379a8e5a1",
            "ad5ec7bd878d40518b2123e1692da520",
            "39552553223640e28fd3544c1e58a8a7",
            "1af3482119f841ddab8075319ae892a1",
            "d56ffcd56a6b4ea0a4d67c4af45861e7",
            "9ae9b29d19f7471aba621f9028607816",
            "d713988d66e04e9b94ce69f9d48a53b0",
            "bc83b69c41684fb7997afd0eb4f8b00d",
            "45a32f64d9454bbe88b37c59c753cb69",
            "a7d604377e82478789678096b4455772",
            "55478e0c8c7340498faf6f376d1e5d1a",
            "a5c3f39cc8464ba891a69d27d65b2071",
            "b410afb7d9f3486ca83a7128a0e9b895",
            "c2ca9daa63f64e3c9da19876aabed539",
            "cc82abd08eed4474b2d7cee8d7c893b1",
            "357c1c325e0f426f91e4bf74bb98f444",
            "e2c35319cd4344de8369f86de469371d",
            "ff05731df373484b990fe820141200c2",
            "1a27ebe4bae8420ab2edc298e22983dc",
            "83f600c1d76a43258479c5a52050b6cd",
            "29f11ceb2ca64647b1b8ff27f2ef55d0",
            "50cf7af0182f45bfabffbac0faf72f81",
            "4b29592f2b2249e8a6e732828c1b4317",
            "7bee1cc3f5344e0ba0b7c74a5a8cf1cb",
            "ca29dc72e7b349d9bc14c7d2c5c75333",
            "85d2d3832fb34e95a2c0467a2601eab1",
            "69b3f80276eb4eff954fd67ee7e91a19",
            "a65041f9287d4ba4adce798431582d0c",
            "9b4c718eab104583ab2f0d68e92434db",
            "41ce4de6ed504f5da174d398268c5a26",
            "71d8256ed8594dfb8ad7ea33160dc2a4",
            "3fe8a3a48867490293312f4fd81e9e81",
            "c92cee4c2b1e40e09cc828673056fddd"
          ]
        },
        "id": "1yxaD-0rEwCP",
        "outputId": "eca573cf-cb45-4152-eb1e-296a41fdf551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f23bc515b0574a1ab59893f28bf00da6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30db659aef8a443395f2c2809dabc0e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/38cc92ec43315abd5136313225e95acc5986876c/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/38cc92ec43315abd5136313225e95acc5986876c/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e07bbd681b44941858dd0e379a8e5a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/38cc92ec43315abd5136313225e95acc5986876c/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/38cc92ec43315abd5136313225e95acc5986876c/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5c3f39cc8464ba891a69d27d65b2071"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/38cc92ec43315abd5136313225e95acc5986876c/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b29592f2b2249e8a6e732828c1b4317"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/38cc92ec43315abd5136313225e95acc5986876c/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "## Your code for initializing the transformer model(s)\n",
        "#\n",
        "# Note that transformer models use their own `tokenizer`, that should be loaded in as well.\n",
        "#\n",
        "from transformers import *\n",
        "import transformers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('distilgpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXBxC6ihEwCQ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "## Your code for initializing the rnn model(s)\n",
        "#\n",
        "# The Gulordava LSTM model can be found here: \n",
        "# https://drive.google.com/file/d/19Lp3AM4NEPycp_IBgoHfLc_V456pmUom/view?usp=sharing\n",
        "# You can read more about this model in the original paper here: https://arxiv.org/pdf/1803.11138.pdf\n",
        "#\n",
        "# N.B: I have altered the RNNModel code to only output the hidden states that you are interested in.\n",
        "# If you want to do more experiments with this model you could have a look at the original code here:\n",
        "# https://github.com/facebookresearch/colorlessgreenRNNs/blob/master/src/language_models/model.py\n",
        "#\n",
        "from collections import defaultdict\n",
        "from lstm.model import RNNModel\n",
        "import torch\n",
        "\n",
        "pre_path = \"/content/drive/My Drive/nlp2-probing-lms/\"\n",
        "# pre_path = \"\"\n",
        "\n",
        "model_location = f'{pre_path}state_dictLSTM.pt'  # <- point this to the location of the Gulordava .pt file\n",
        "lstm = RNNModel('LSTM', 50001, 650, 650, 2)\n",
        "lstm.load_state_dict(torch.load(model_location))\n",
        "\n",
        "\n",
        "# This LSTM does not use a Tokenizer like the Transformers, but a Vocab dictionary that maps a token to an id.\n",
        "with open(f'{pre_path}lstm/vocab.txt', encoding=\"utf8\") as f:\n",
        "    w2i = {w.strip(): i for i, w in enumerate(f)}\n",
        "\n",
        "vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
        "vocab.update(w2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNAWS0P-EwCR"
      },
      "outputs": [],
      "source": [
        "# READ DATA\n",
        "from typing import List\n",
        "from conllu import parse_incr, TokenList\n",
        "\n",
        "\n",
        "# If stuff like `: str` and `-> ..` seems scary, fear not! \n",
        "# These are type hints that help you to understand what kind of argument and output is expected.\n",
        "def parse_corpus(filename: str) -> List[TokenList]:\n",
        "    data_file = open(filename, encoding=\"utf-8\")\n",
        "\n",
        "    ud_parses = list(parse_incr(data_file))\n",
        "    # Filter out short sentences\n",
        "    ud_parses = [parse for parse in ud_parses if len(parse) >= 5]\n",
        "    \n",
        "    return ud_parses\n",
        "\n",
        "\n",
        "ud_parses = parse_corpus(f'{pre_path}data/sample/en_ewt-ud-train.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPikUzvAEwCR"
      },
      "outputs": [],
      "source": [
        "# FETCH SENTENCE REPRESENTATIONS\n",
        "from torch import Tensor\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Should return a tensor of shape (num_tokens_in_corpus, representation_size)\n",
        "# Make sure you correctly average the subword representations that belong to 1 token!\n",
        "\n",
        "def fetch_sen_reps(ud_parses: List[TokenList], model, tokenizer, concat=True, layer=-1) -> Tensor:\n",
        "    \n",
        "    model.eval()\n",
        "    # Initialize an empty list to store sentence representations\n",
        "    corpus_rep = []\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Transformer model\n",
        "    if isinstance(tokenizer, transformers.PreTrainedTokenizer):\n",
        "        # Loop through the parses\n",
        "        for parse in tqdm(ud_parses):\n",
        "            prev_token = None\n",
        "            encoded_full = []\n",
        "            encoded = []\n",
        "            tokens = []\n",
        "            # Loop through the tokens in the parse\n",
        "            for token in parse:\n",
        "                # Check if the previous token has a misc attribute and SpaceAfter key\n",
        "                if prev_token is None or ('misc' in prev_token and prev_token['misc'] is not None and 'SpaceAfter' in prev_token['misc'] and prev_token['misc']['SpaceAfter'] == 'No'):\n",
        "                    tokens.append(token['form'])\n",
        "                else:\n",
        "                    tokens.append(' ' + token['form'])\n",
        "                prev_token = token\n",
        "\n",
        "            # Encode each word using the tokenizer\n",
        "            for token in tokens:\n",
        "                with torch.no_grad():\n",
        "                    encoded_token = tokenizer.encode(token, add_special_tokens=False)\n",
        "                    encoded.extend(encoded_token)\n",
        "                    encoded_full.append(encoded_token)\n",
        "\n",
        "            # Convert the encoded subword to a tensor        \n",
        "            input_ids = torch.tensor(encoded).unsqueeze(0)\n",
        "            input_ids = input_ids.to(device)\n",
        "            # Get the sentence representation from the model's output\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, output_hidden_states=True)\n",
        "\n",
        "            last_layer_hidden_state = outputs.hidden_states[layer]\n",
        "            sen_rep = last_layer_hidden_state.squeeze()\n",
        "            # Fix for one token sentences\n",
        "            if len(parse) <= 1:\n",
        "                sen_rep = sen_rep.unsqueeze(0)\n",
        "            # Average the representations of subwords\n",
        "            sen_rep_avg = []\n",
        "            i = 0\n",
        "            for word in encoded_full:\n",
        "                j = len(word)\n",
        "                sen_rep_avg.append(torch.mean(sen_rep[i:i+j], dim=0, keepdim=True))\n",
        "                i += j\n",
        "            tensor = torch.stack(sen_rep_avg, dim=0).squeeze(1)\n",
        "            if tensor.shape[0] == 1:\n",
        "                tensor = tensor.view(1, -1, tensor.shape[-1])[:, 0, :]      \n",
        "            corpus_rep.append(tensor)\n",
        "        \n",
        "    # LSTM model\n",
        "    else:\n",
        "        for parse in ud_parses:\n",
        "            encoded = [tokenizer.get(token['form'].replace(' ', ''), tokenizer['<unk>']) for token in parse]\n",
        "            input_ids = torch.tensor(encoded).unsqueeze(0)\n",
        "            input_ids = input_ids.to(device)\n",
        "            hidden = model.init_hidden(1)\n",
        "            with torch.no_grad():\n",
        "                tensor = lstm(input_ids, hidden).squeeze()\n",
        "            if len(tensor.shape) == 1:\n",
        "                tensor = tensor.unsqueeze(0)\n",
        "            corpus_rep.append(tensor)\n",
        "    if concat:\n",
        "        corpus_rep = torch.cat(corpus_rep, dim=0)\n",
        "    \n",
        "    return corpus_rep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKOGY--7EwCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c2b7b9-dddc-4ea2-cd8f-26f791eb492d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n"
          ]
        }
      ],
      "source": [
        "def error_msg(model_name, gold_embs, embs, i2w):\n",
        "    with open(f'{pre_path}{model_name}_tokens1.pickle', 'rb') as f:\n",
        "        sen_tokens = pickle.load(f)\n",
        "        \n",
        "    diff = torch.abs(embs - gold_embs)\n",
        "    max_diff = torch.max(diff)\n",
        "    avg_diff = torch.mean(diff)\n",
        "    \n",
        "    print(f\"{model_name} embeddings don't match!\")\n",
        "    print(f\"Max diff.: {max_diff:.4f}\\nMean diff. {avg_diff:.4f}\")\n",
        "\n",
        "    print(\"\\nCheck if your tokenization matches with the original tokenization:\")\n",
        "    for idx in sen_tokens.squeeze():\n",
        "        if isinstance(i2w, list):\n",
        "            token = i2w[idx]\n",
        "        else:\n",
        "            token = i2w.convert_ids_to_tokens(idx.item())\n",
        "        print(f\"{idx:<6} {token}\")\n",
        "\n",
        "\n",
        "def assert_sen_reps(model, tokenizer, lstm, vocab):\n",
        "    with open(f'{pre_path}distilgpt2_emb1.pickle', 'rb') as f:\n",
        "        distilgpt2_emb1 = pickle.load(f)\n",
        "        \n",
        "    with open(f'{pre_path}lstm_emb1.pickle', 'rb') as f:\n",
        "        lstm_emb1 = pickle.load(f)\n",
        "    \n",
        "    corpus = parse_corpus(f'{pre_path}data/sample/en_ewt-ud-train.conllu')[:1]\n",
        "    \n",
        "    own_distilgpt2_emb1 = fetch_sen_reps(corpus, model, tokenizer)\n",
        "    own_lstm_emb1 = fetch_sen_reps(corpus, lstm, vocab)\n",
        "    \n",
        "    # assert distilgpt2_emb1.shape == own_distilgpt2_emb1.shape, \\\n",
        "    #     f\"Distilgpt2 shape mismatch: {distilgpt2_emb1.shape} (gold) vs. {own_distilgpt2_emb1.shape} (yours)\"\n",
        "    # assert lstm_emb1.shape == own_lstm_emb1.shape, \\\n",
        "    #     f\"LSTM shape mismatch: {lstm_emb1.shape} (gold) vs. {own_lstm_emb1.shape} (yours)\"\n",
        "\n",
        "    # if not torch.allclose(distilgpt2_emb1, own_distilgpt2_emb1, rtol=1e-3, atol=1e-3):\n",
        "    #     error_msg(\"distilgpt2\", distilgpt2_emb1, own_distilgpt2_emb1, tokenizer)\n",
        "    # if not torch.allclose(lstm_emb1, own_lstm_emb1, rtol=1e-3, atol=1e-3):\n",
        "    #     error_msg(\"lstm\", lstm_emb1, own_lstm_emb1, list(vocab.keys()))\n",
        "\n",
        "\n",
        "assert_sen_reps(model, tokenizer, lstm, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_NTSvmLEwCS",
        "outputId": "fa0f2390-5c79-4a76-d332-3a759774046c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 0,  ..., 4, 0, 1]),\n",
              " {'PROPN': 0,\n",
              "  'PUNCT': 1,\n",
              "  'ADJ': 2,\n",
              "  'NOUN': 3,\n",
              "  'VERB': 4,\n",
              "  'DET': 5,\n",
              "  'ADP': 6,\n",
              "  'AUX': 7,\n",
              "  'PRON': 8,\n",
              "  'PART': 9,\n",
              "  'SCONJ': 10,\n",
              "  'NUM': 11,\n",
              "  'ADV': 12,\n",
              "  'CCONJ': 13})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# FETCH POS LABELS\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "# Should return a tensor of shape (num_tokens_in_corpus,)\n",
        "# Make sure that when fetching these pos tags for your train/dev/test corpora you share the label vocabulary.\n",
        "def fetch_pos_tags(ud_parses: List[TokenList], pos_vocab: Optional[Dict[str, int]] = None) -> Tuple[Tensor, Dict[str, int]]:\n",
        "    \n",
        "    # Create label dictionary if not passed to function\n",
        "    if pos_vocab is None:\n",
        "        pos_vocab = {}\n",
        "        pos_counter = 0\n",
        "    else:\n",
        "        pos_counter = len(pos_vocab.values())\n",
        "\n",
        "    # Keep a list of all POS tag labels\n",
        "    all_tags = []\n",
        "\n",
        "    for parse in ud_parses:\n",
        "        for token in parse:\n",
        "            # Extract the POS tag for the given token\n",
        "            pos_tag = token['upostag']\n",
        "            \n",
        "            # If the token is not yet in the vocabulary, add it\n",
        "            if pos_tag not in pos_vocab.keys():\n",
        "                pos_vocab[pos_tag] = pos_counter\n",
        "                pos_counter += 1\n",
        "            \n",
        "            # Add the label encoding of the POS tag\n",
        "            all_tags.append(pos_vocab[pos_tag])\n",
        "\n",
        "    return torch.tensor(all_tags), pos_vocab\n",
        "\n",
        "\n",
        "fetch_pos_tags(ud_parses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzc-4t7XEwCT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Function that combines the previous functions, and creates 2 tensors for a .conllu file: \n",
        "# 1 containing the token representations, and 1 containing the (tokenized) pos_tags.\n",
        "\n",
        "def create_data(ud_parses, filename: str, lm, w2i, pos_vocab=None, layer=-1):\n",
        "    new_parses = parse_corpus(filename)\n",
        "    \n",
        "    sen_reps = fetch_sen_reps(new_parses, lm, w2i, concat=False, layer=layer)\n",
        "    pos_tags, pos_vocab = fetch_pos_tags(new_parses, pos_vocab=pos_vocab)\n",
        "    \n",
        "    return sen_reps, pos_tags, pos_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwgsTqqyEwCT"
      },
      "source": [
        "# Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBz0d2zpEwCT"
      },
      "outputs": [],
      "source": [
        "# In case you want to transform your conllu tree to an nltk.Tree, for better visualisation\n",
        "\n",
        "def rec_tokentree_to_nltk(tokentree):\n",
        "    token = tokentree.token[\"form\"]\n",
        "    tree_str = f\"({token} {' '.join(rec_tokentree_to_nltk(t) for t in tokentree.children)})\"\n",
        "\n",
        "    return tree_str\n",
        "\n",
        "\n",
        "def tokentree_to_nltk(tokentree):\n",
        "    from nltk import Tree as NLTKTree\n",
        "\n",
        "    tree_str = rec_tokentree_to_nltk(tokentree)\n",
        "\n",
        "    return NLTKTree.fromstring(tree_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7WFVgNxEwCT",
        "outputId": "506a3dfd-a400-4a61-bcb3-c988f915ede4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ete3\n",
            "  Downloading ete3-3.1.3.tar.gz (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ete3\n",
            "  Building wheel for ete3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ete3: filename=ete3-3.1.3-py3-none-any.whl size=2273786 sha256=5149d4b4f742cef3ed87eb69f384771d6878405cd274522bc08f072bffae1f74\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/72/00/1982bd848e52b03079dbf800900120bc1c20e92e9a1216e525\n",
            "Successfully built ete3\n",
            "Installing collected packages: ete3\n",
            "Successfully installed ete3-3.1.3\n"
          ]
        }
      ],
      "source": [
        "%pip install ete3\n",
        "from ete3 import Tree as EteTree\n",
        "\n",
        "\n",
        "class FancyTree(EteTree):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, format=1, **kwargs)\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.get_ascii(show_internal=True)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "\n",
        "def rec_tokentree_to_ete(tokentree):\n",
        "    idx = str(tokentree.token[\"id\"])\n",
        "    children = tokentree.children\n",
        "    if children:\n",
        "        return f\"({','.join(rec_tokentree_to_ete(t) for t in children)}){idx}\"\n",
        "    else:\n",
        "        return idx\n",
        "    \n",
        "def tokentree_to_ete(tokentree):\n",
        "    newick_str = rec_tokentree_to_ete(tokentree)\n",
        "\n",
        "    return FancyTree(f\"{newick_str};\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jcnzC7eEwCU",
        "outputId": "ed9eed22-6750-42ec-b81f-016d0097a772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   /-2\n",
            "  |\n",
            "  |--3\n",
            "  |\n",
            "  |--4\n",
            "  |\n",
            "  |   /6 /-5\n",
            "  |  |\n",
            "  |  |   /-9\n",
            "  |  |  |\n",
            "  |  |  |--10\n",
            "  |  |  |\n",
            "  |  |  |--11\n",
            "  |  |-8|\n",
            "  |  |  |--12\n",
            "  |-7|  |\n",
            "  |  |  |--13\n",
            "  |  |  |\n",
            "  |  |   \\15/-14\n",
            "-1|  |\n",
            "  |  |   /-16\n",
            "  |  |  |\n",
            "  |  |  |--17\n",
            "  |  |  |\n",
            "  |   \\18   /-19\n",
            "  |     |  |\n",
            "  |     |  |--20\n",
            "  |     |  |\n",
            "  |     |  |-23/-22\n",
            "  |      \\21\n",
            "  |        |--24\n",
            "  |        |\n",
            "  |        |   /-25\n",
            "  |        |  |\n",
            "  |         \\28--26\n",
            "  |           |\n",
            "  |            \\-27\n",
            "  |\n",
            "   \\-29\n"
          ]
        }
      ],
      "source": [
        "# Let's check if it works!\n",
        "# We can read in a corpus using the code that was already provided, and convert it to an ete3 Tree.\n",
        "\n",
        "def parse_corpus(filename):\n",
        "    from conllu import parse_incr\n",
        "\n",
        "    data_file = open(filename, encoding=\"utf-8\")\n",
        "\n",
        "    ud_parses = list(parse_incr(data_file))\n",
        "    \n",
        "    return ud_parses\n",
        "\n",
        "corpus = parse_corpus(f'{pre_path}data/sample/en_ewt-ud-train.conllu')\n",
        "item = corpus[0]\n",
        "tokentree = item.to_tree()\n",
        "ete3_tree = tokentree_to_ete(tokentree)\n",
        "print(ete3_tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu2ms3V4EwCU"
      },
      "outputs": [],
      "source": [
        "def create_gold_distances(corpus):\n",
        "    all_distances = []\n",
        "\n",
        "    for item in corpus:\n",
        "        tokentree = item.to_tree()\n",
        "        ete_tree = tokentree_to_ete(tokentree)\n",
        "\n",
        "        sen_len = len(ete_tree.search_nodes())\n",
        "        distances = torch.zeros((sen_len, sen_len))\n",
        "\n",
        "        # Compute all the distances\n",
        "        for i in range(sen_len):\n",
        "            for j in range(sen_len):\n",
        "                if i == j:\n",
        "                    distances[i,j] = 0.0\n",
        "                    distances[j,i] = 0.0\n",
        "                else:\n",
        "                    distances[i,j] = ete_tree.get_distance(str(i+1), str(j+1))\n",
        "                    distances[j,i] = ete_tree.get_distance(str(i+1), str(j+1))\n",
        "\n",
        "        all_distances.append(distances)\n",
        "\n",
        "    return all_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nM3bqELEwCU"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "import torch\n",
        "\n",
        "\n",
        "def create_mst(distances):\n",
        "    distances = torch.triu(distances).detach().cpu().numpy()\n",
        "    mst = minimum_spanning_tree(distances).toarray()\n",
        "    mst[mst>0] = 1.\n",
        "    \n",
        "    return mst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIgC-rmtEwCU",
        "outputId": "cfbb6a34-e9ba-438f-f77e-2cd28db247bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   /2 /-1\n",
            "  |\n",
            "  |--3\n",
            "  |\n",
            "  |--4\n",
            "  |\n",
            "  |   /-6\n",
            "  |  |\n",
            "-5|  |--7\n",
            "  |-8|\n",
            "  |  |   /-9\n",
            "  |  |  |\n",
            "  |   \\12--10\n",
            "  |     |\n",
            "  |      \\-11\n",
            "  |\n",
            "   \\-13 \n",
            "\n",
            "tensor([[0., 1., 3., 3., 2., 4., 4., 3., 5., 5., 5., 4., 3.],\n",
            "        [1., 0., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [3., 2., 0., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [3., 2., 2., 0., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [2., 1., 1., 1., 0., 2., 2., 1., 3., 3., 3., 2., 1.],\n",
            "        [4., 3., 3., 3., 2., 0., 2., 1., 3., 3., 3., 2., 3.],\n",
            "        [4., 3., 3., 3., 2., 2., 0., 1., 3., 3., 3., 2., 3.],\n",
            "        [3., 2., 2., 2., 1., 1., 1., 0., 2., 2., 2., 1., 2.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 0., 2., 2., 1., 4.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 0., 2., 1., 4.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 2., 0., 1., 4.],\n",
            "        [4., 3., 3., 3., 2., 2., 2., 1., 1., 1., 1., 0., 3.],\n",
            "        [3., 2., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 0.]]) \n",
            "\n",
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([[0., 1., 3., 3., 2., 4., 4., 3., 5., 5., 5., 4., 3.],\n",
            "        [1., 0., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [3., 2., 0., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [3., 2., 2., 0., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [2., 1., 1., 1., 0., 2., 2., 1., 3., 3., 3., 2., 1.],\n",
            "        [4., 3., 3., 3., 2., 0., 2., 1., 3., 3., 3., 2., 3.],\n",
            "        [4., 3., 3., 3., 2., 2., 0., 1., 3., 3., 3., 2., 3.],\n",
            "        [3., 2., 2., 2., 1., 1., 1., 0., 2., 2., 2., 1., 2.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 0., 2., 2., 1., 4.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 0., 2., 1., 4.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 2., 0., 1., 4.],\n",
            "        [4., 3., 3., 3., 2., 2., 2., 1., 1., 1., 1., 0., 3.],\n",
            "        [3., 2., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 0.]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "item = corpus[5]\n",
        "tokentree = item.to_tree()\n",
        "ete3_tree = tokentree_to_ete(tokentree)\n",
        "print(ete3_tree, '\\n')\n",
        "\n",
        "gold_distance = create_gold_distances(corpus[5:6])[0]\n",
        "print(gold_distance, '\\n')\n",
        "\n",
        "mst = create_mst(gold_distance)\n",
        "print(mst)\n",
        "print(gold_distance, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ctxxb3LSEwCV"
      },
      "outputs": [],
      "source": [
        "def edges(mst):\n",
        "    # Your code for retrieving the edges from the MST matrix\n",
        "    edges = set()\n",
        "    for i, node1 in enumerate(mst):\n",
        "        for j, node2 in enumerate(node1):\n",
        "            if node2 == 1:\n",
        "                edges.add((i,j))\n",
        "                edges.add((j,i))\n",
        "\n",
        "    return edges\n",
        "\n",
        "def calc_uuas(pred_distances, gold_distances):\n",
        "    uuas = None\n",
        "    # Find the indices of non-padded rows and columns\n",
        "    non_padded_rows = torch.any(gold_distances != -1, dim=1)\n",
        "    non_padded_cols = torch.any(gold_distances != -1, dim=0)\n",
        "\n",
        "    # Slice both tensors using the non-padded indices\n",
        "    pred_distances = pred_distances[non_padded_rows][:, non_padded_cols]\n",
        "    gold_distances = gold_distances[non_padded_rows][:, non_padded_cols]\n",
        "    \n",
        "    pred_mst = create_mst(pred_distances)\n",
        "    gold_mst = create_mst(gold_distances)\n",
        "    pred_edges = edges(pred_mst)\n",
        "    gold_edges = edges(gold_mst)\n",
        "    correct = pred_edges.intersection(gold_edges)\n",
        "    if len(gold_edges) == 0:\n",
        "        uuas = 1\n",
        "    else:\n",
        "        uuas = len(correct) / len(gold_edges)\n",
        "    return uuas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVfrZG_-EwCV"
      },
      "source": [
        "# Structural probing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQk736JxEwCV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class StructuralProbe(nn.Module):\n",
        "    \"\"\" Computes squared L2 distance after projection by a matrix.\n",
        "    For a batch of sentences, computes all n^2 pairs of distances\n",
        "    for each sentence in the batch.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_dim, rank, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.probe_rank = rank\n",
        "        self.model_dim = model_dim\n",
        "        \n",
        "        self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
        "        self.proj_norms = nn.Parameter(data=torch.zeros(self.model_dim, self.probe_rank))\n",
        "        \n",
        "        nn.init.uniform_(self.proj, -0.05, 0.05)\n",
        "        nn.init.uniform_(self.proj_norms, -0.05, 0.05)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\" Computes all n^2 pairs of distances after projection\n",
        "        for each sentence in a batch.\n",
        "        Note that due to padding, some distances will be non-zero for pads.\n",
        "        Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
        "        Args:\n",
        "          batch: a batch of word representations of the shape\n",
        "            (batch_size, max_seq_len, representation_dim)\n",
        "        Returns:\n",
        "          A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
        "        \"\"\"\n",
        "        transformed = torch.matmul(batch, self.proj)\n",
        "        transformed_norms = torch.matmul(batch, self.proj_norms)\n",
        "        \n",
        "        batchlen, seqlen, rank = transformed.size()\n",
        "        \n",
        "        transformed = transformed.unsqueeze(2)\n",
        "        transformed = transformed.expand(-1, -1, seqlen, -1)\n",
        "        transposed = transformed.transpose(1,2)\n",
        "        \n",
        "        diffs = transformed - transposed\n",
        "        \n",
        "        squared_diffs = diffs.pow(2)\n",
        "        squared_distances = torch.sum(squared_diffs, -1)\n",
        "        \n",
        "        squared_norms = transformed_norms.pow(2).sum(-1)\n",
        "\n",
        "        return squared_distances, squared_norms\n",
        "\n",
        "    \n",
        "    \n",
        "class L1DistanceLoss(nn.Module):\n",
        "    \"\"\"Custom L1 loss for distance matrices.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predictions, label_batch, length_batch):\n",
        "        \"\"\" Computes L1 loss on distance matrices.\n",
        "        Ignores all entries where label_batch=-1\n",
        "        Normalizes first within sentences (by dividing by the square of the sentence length)\n",
        "        and then across the batch.\n",
        "        Args:\n",
        "          predictions: A pytorch batch of predicted distances\n",
        "          label_batch: A pytorch batch of true distances\n",
        "          length_batch: A pytorch batch of sentence lengths\n",
        "        Returns:\n",
        "          A tuple of:\n",
        "            batch_loss: average loss in the batch\n",
        "            total_sents: number of sentences in the batch\n",
        "        \"\"\"\n",
        "        labels_1s = (label_batch != -1).float()\n",
        "        predictions_masked = predictions * labels_1s\n",
        "        labels_masked = label_batch * labels_1s\n",
        "        total_sents = torch.sum((length_batch != 0)).float()\n",
        "        squared_lengths = length_batch.pow(2).float()\n",
        "\n",
        "        if total_sents > 0:\n",
        "            loss_per_sent = torch.sum(torch.abs(predictions_masked - labels_masked), dim=(1,2))\n",
        "            normalized_loss_per_sent = loss_per_sent / squared_lengths\n",
        "            batch_loss = torch.sum(normalized_loss_per_sent) / total_sents\n",
        "        \n",
        "        else:\n",
        "            batch_loss = torch.tensor(0.0)\n",
        "        \n",
        "        return batch_loss, total_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CzYp96BEwCV"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "import math\n",
        "\n",
        "'''\n",
        "Similar to the `create_data` method of the previous notebook, I recommend you to use a method \n",
        "that initialises all the data of a corpus. Note that for your embeddings you can use the \n",
        "`fetch_sen_reps` method again. However, for the POS probe you concatenated all these representations into \n",
        "1 big tensor of shape (num_tokens_in_corpus, model_dim). \n",
        "\n",
        "The StructuralProbe expects its input to contain all the representations of 1 sentence, so I recommend you\n",
        "to update your `fetch_sen_reps` method in a way that it is easy to retrieve all the representations that \n",
        "correspond to a single sentence.\n",
        "''' \n",
        "\n",
        "def init_corpus(path, concat=False, cutoff=None, layer=-1):\n",
        "    \"\"\" Initialises the data of a corpus.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        Path to corpus location\n",
        "    concat : bool, optional\n",
        "        Optional toggle to concatenate all the tensors\n",
        "        returned by `fetch_sen_reps`.\n",
        "    cutoff : int, optional\n",
        "        Optional integer to \"cutoff\" the data in the corpus.\n",
        "        This allows only a subset to be used, alleviating \n",
        "        memory usage.\n",
        "    \"\"\"\n",
        "    corpus = parse_corpus(path)[:cutoff]\n",
        "    embs = fetch_sen_reps(corpus, lstm, vocab, concat=concat, layer=layer)\n",
        "#     embs = fetch_sen_reps(corpus, model, tokenizer, concat=concat, layer=layer)    \n",
        "    gold_distances = create_gold_distances(corpus)\n",
        "    \n",
        "    return gold_distances, embs\n",
        "\n",
        "\n",
        "# I recommend you to write a method that can evaluate the UUAS & loss score for the dev (& test) corpus.\n",
        "# Feel free to alter the signature of this method.\n",
        "def evaluate_probe(probe, _data, device):\n",
        "    loss_function =  L1DistanceLoss()\n",
        "    probe.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        distances = _data[0]\n",
        "        embs = _data[1]\n",
        "        \n",
        "        # Make big tensor of shape (batch_size, max_seq_len, representation_dim)\n",
        "        # from the representations in the batch using zero-padding\n",
        "        max_seq_len = max([tensor.shape[0] for tensor in embs])\n",
        "        batch_size = len(embs)\n",
        "        representation_dim = embs[0].shape[1]\n",
        "        embs_tensor = torch.zeros(batch_size, max_seq_len, representation_dim)\n",
        "        for i, tensor in enumerate(embs):\n",
        "            if tensor.shape[0] == 1:\n",
        "                tensor = tensor.view(1, -1, representation_dim)[:, 0, :]\n",
        "            seq_len = tensor.shape[0]\n",
        "            embs_tensor[i, :seq_len] = tensor\n",
        "\n",
        "        # Predict distances and norms using probe\n",
        "        predicted_distances, predicted_norms = probe(embs_tensor.to(device))\n",
        "\n",
        "        # Make big tensor of distances from batch padded with -1\n",
        "        # resulting in true_distances of shape (batch_size, max_seq_len, max_seq_len)\n",
        "        max_seq_len = max([tensor.shape[0] for tensor in distances])\n",
        "        batch_size = len(distances)\n",
        "        sentence_lengths = torch.zeros(batch_size, dtype=torch.long)\n",
        "        true_distances = torch.full((batch_size, max_seq_len, max_seq_len), -1)\n",
        "        for i, tensor in enumerate(distances):\n",
        "            seq_len = tensor.shape[0]\n",
        "            true_distances[i, :seq_len, :seq_len] = tensor\n",
        "            sentence_lengths[i] = seq_len\n",
        "\n",
        "        # Calculate loss and uass score\n",
        "        loss, total_sents = loss_function(predicted_distances.to(device), true_distances.to(device), sentence_lengths.to(device))\n",
        "        uuas_score = [calc_uuas(pred, true) for pred, true in zip(predicted_distances, true_distances)]\n",
        "    \n",
        "    return loss.item(), uuas_score\n",
        "\n",
        "\n",
        "# Feel free to alter the signature of this method.\n",
        "def train(_train_data, _dev_data, _test_data, device, epochs=1000):\n",
        "    emb_dim = _train_data[1][0].shape[-1]\n",
        "    rank = 64\n",
        "    lr = 10e-4\n",
        "    batch_size = 24\n",
        "    dev_losses = []\n",
        "    dev_uuases = []\n",
        "\n",
        "    probe = StructuralProbe(emb_dim, rank, device)\n",
        "    optimizer = optim.Adam(probe.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,patience=1)\n",
        "    loss_function =  L1DistanceLoss()\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        for i in range(0, len(_train_data[1]), batch_size):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # YOUR CODE FOR DOING A PROBE FORWARD PASS\n",
        "            batch_embs = _train_data[1][i:i+batch_size]\n",
        "            batch_distances = _train_data[0][i:i+batch_size]\n",
        "            \n",
        "            \n",
        "            # Make big tensor of shape (batch_size, max_seq_len, representation_dim)\n",
        "            # from the representations in the batch using zero-padding\n",
        "            max_seq_len = max([tensor.shape[0] for tensor in batch_embs])\n",
        "            embs_tensor = torch.zeros(len(batch_embs), max_seq_len, emb_dim)\n",
        "            for j, tensor in enumerate(batch_embs):\n",
        "                if tensor.shape[0] == 1:\n",
        "                    tensor = tensor.view(1, -1, emb_dim)[:, 0, :]\n",
        "                seq_len = tensor.shape[0]\n",
        "                embs_tensor[j, :seq_len] = tensor\n",
        "    \n",
        "            # Predict distances and using probe\n",
        "            predicted_distances, predicted_norms = probe(embs_tensor.to(device))\n",
        "            \n",
        "            # Make big tensor of shape (batch_size, max_seq_len, max_seq_len) from distances\n",
        "            # in the batch using padding with -1\n",
        "            max_seq_len = max([tensor.shape[0] for tensor in batch_distances])\n",
        "            sentence_lengths = torch.zeros(len(batch_embs), dtype=torch.long)\n",
        "            true_distances = torch.full((len(batch_embs), max_seq_len, max_seq_len), -1)\n",
        "            for j, tensor in enumerate(batch_distances):\n",
        "                seq_len = tensor.shape[0]\n",
        "                true_distances[j, :seq_len, :seq_len] = tensor\n",
        "                sentence_lengths[j] = seq_len\n",
        "\n",
        "            # Calculate loss\n",
        "            batch_loss, total_sents = loss_function(predicted_distances.to(device), true_distances.to(device), sentence_lengths.to(device))\n",
        "\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        dev_loss, dev_uuas = evaluate_probe(probe, _dev_data, device)\n",
        "        dev_losses.append(dev_loss)\n",
        "        dev_uuases.append(sum(dev_uuas)/len(dev_uuas))\n",
        "        \n",
        "\n",
        "        # Using a scheduler is up to you, and might require some hyper param fine-tuning\n",
        "        scheduler.step(dev_loss)\n",
        "\n",
        "    test_loss, test_uuas = evaluate_probe(probe, _test_data, device)\n",
        "    return dev_losses, dev_uuases, test_loss, sum(test_uuas)/len(test_uuas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qagGv0mdEwCV"
      },
      "source": [
        "# Amnesic probing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAzZ-AenEwCW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "def get_probe_projection(probe):\n",
        "    \"\"\"\n",
        "    Function to get the nullspace projection of a linear probe\n",
        "    \"\"\"\n",
        "\n",
        "    null_space_proj = scipy.linalg.null_space(probe)\n",
        "    return null_space_proj\n",
        "\n",
        "\n",
        "def project_representations(data, probe):\n",
        "    \"\"\"\n",
        "    Function to project all training data on the null space of the probe,\n",
        "    as such effectively removing all information conveyed by the probe\n",
        "    \"\"\"\n",
        "    # print(f\"Probe shape: {probe.shape}\")\n",
        "    # print(f\"Data shape: {data.shape}\")\n",
        "\n",
        "    null_space_proj = get_probe_projection(probe)\n",
        "    # print(f\"Null space projection shape: {null_space_proj.shape}\")\n",
        "    # print(f\"Null space projection: {null_space_proj}\")\n",
        "    # print(f\"Vector shape: {first_vec.shape}\")\n",
        "    # print(f\"Projection shape: {proj.shape}\")\n",
        "\n",
        "    projected_data = [torch.from_numpy(np.array(elem.cpu())@null_space_proj) for elem in data]\n",
        "\n",
        "    # print(f\"full projection shape: {projected_data.shape}\")\n",
        "    # print(f\"projection: {projected_data}\")\n",
        "    \n",
        "    return projected_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsr1hAZnEwCW"
      },
      "outputs": [],
      "source": [
        "def get_struct_data(path):\n",
        "\n",
        "    corpus = parse_corpus(path)\n",
        "    gold_distances = create_gold_distances(corpus)\n",
        "    \n",
        "    return gold_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84jH05hpEwCW",
        "outputId": "631c68e2-6d8c-4a30-8ed8-0948664eaf96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating training distances...\n",
            "Done generating training distances. This took 376.75759983062744s\n",
            "Generating validation distances...\n",
            "Done generating validation distances. This took 32.499839305877686s\n",
            "Generating testing distances...\n",
            "Done generating testing distances. This took 33.48542618751526s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "num_layers = 7\n",
        "epochs = 20\n",
        "test_UUAS_per_layer = []\n",
        "use_sample = False\n",
        "model_name = \"GPT\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "conllu_path = \"/content/drive/My Drive/nlp2-probing-lms/data/\"\n",
        "# conllu_path = \"data/\"\n",
        "\n",
        "if use_sample:\n",
        "  conllu_path += \"sample/\"\n",
        "\n",
        "t = time.time()\n",
        "print(f\"Generating training distances...\")\n",
        "gold_distances_train = get_struct_data(f\"{conllu_path}en_ewt-ud-train.conllu\")\n",
        "print(f\"Done generating training distances. This took {time.time() - t}s\")\n",
        "\n",
        "t = time.time()\n",
        "print(f\"Generating validation distances...\")\n",
        "gold_distances_dev = get_struct_data(f\"{conllu_path}en_ewt-ud-dev.conllu\")\n",
        "print(f\"Done generating validation distances. This took {time.time() - t}s\")\n",
        "\n",
        "t = time.time()\n",
        "print(f\"Generating testing distances...\")\n",
        "gold_distances_test = get_struct_data(f\"{conllu_path}en_ewt-ud-test.conllu\")\n",
        "print(f\"Done generating testing distances. This took {time.time() - t}s\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    print(f\"Examining layer {layer}...\")\n",
        "    t = time.time()\n",
        "    print(f\"Generating training data...\")\n",
        "    GPT_train_x, GPT_train_y, GPT_train_vocab = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
        "        model, \n",
        "        tokenizer,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating training data. This took {time.time() - t}s\")\n",
        "\n",
        "    t = time.time()\n",
        "    print(f\"Generating validation data...\")\n",
        "    GPT_dev_x, GPT_dev_y, _ = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
        "        model, \n",
        "        tokenizer,\n",
        "        pos_vocab=GPT_train_vocab,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating validation data. This took {time.time() - t}s\")\n",
        "\n",
        "    t = time.time()\n",
        "    print(f\"Generating test data...\")\n",
        "    GPT_test_x, GPT_test_y, _ = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
        "        model,\n",
        "        tokenizer,\n",
        "        pos_vocab=GPT_train_vocab,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating test data. This took {time.time() - t}s\")\n",
        "\n",
        "\n",
        "    POS_probe = LogisticRegression()\n",
        "    t = time.time()\n",
        "    print(f\"Fitting probe...\")\n",
        "    POS_probe.fit(torch.cat(GPT_train_x, dim=0).cpu(), GPT_train_y)\n",
        "    print(f\"Done fitting POS probe. This took {time.time() - t}s\")\n",
        "\n",
        "    probe_weights = POS_probe.coef_\n",
        "\n",
        "    # Get the \"amnesic vectors\", the representations with POS information removed\n",
        "    GPT_train_projected = project_representations(GPT_train_x, probe_weights)\n",
        "    GPT_dev_projected = project_representations(GPT_dev_x, probe_weights)\n",
        "    GPT_test_projected = project_representations(GPT_test_x, probe_weights)\n",
        "\n",
        "    t = time.time()\n",
        "    print(\"Training structural probe...\")\n",
        "    # Train the structural probe on these amnesic vectors\n",
        "    _, _, proj_GPT_test_loss, proj_GPT_test_uuas = train((gold_distances_train, GPT_train_projected),\n",
        "                                                        (gold_distances_dev, GPT_dev_projected),\n",
        "                                                        (gold_distances_test, GPT_test_projected),\n",
        "                                                        device,\n",
        "                                                        epochs)\n",
        "    print(f\"Done training structural probe. This took {time.time() - t}s\")\n",
        "    print(f\"Layer {layer} test UUAS: {proj_GPT_test_uuas}\")\n",
        "\n",
        "    test_UUAS_per_layer.append(proj_GPT_test_uuas)\n",
        "\n",
        "    print(\"Deleting all data from memory...\")\n",
        "    del GPT_train_x\n",
        "    del GPT_train_y\n",
        "    del GPT_train_vocab\n",
        "    del GPT_dev_x\n",
        "    del GPT_dev_y\n",
        "    del GPT_test_x\n",
        "    del GPT_test_y\n",
        "    del POS_probe\n",
        "    del GPT_train_projected\n",
        "    del GPT_dev_projected\n",
        "    del GPT_test_projected\n",
        "    print(\"Done deleting data from memory. Moving on.\")\n",
        "  \n",
        "print(test_UUAS_per_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yzjwot1KcYHX",
        "outputId": "57da7a60-072a-4818-ff11-f52d44f03fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examining layer 0...\n",
            "Generating training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12543/12543 [02:05<00:00, 99.98it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done generating training data. This took 136.25765323638916s\n",
            "Generating validation data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2002/2002 [00:16<00:00, 121.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done generating validation data. This took 17.016409158706665s\n",
            "Generating test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2077/2077 [00:17<00:00, 121.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done generating test data. This took 17.648144006729126s\n",
            "Fitting probe...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done fitting POS probe. This took 132.1714117527008s\n",
            "Training structural probe...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [00:06<02:06,  6.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [00:11<01:42,  5.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [00:16<01:32,  5.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [00:22<01:30,  5.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [00:27<01:21,  5.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [00:33<01:18,  5.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [00:39<01:12,  5.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [00:44<01:05,  5.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [00:50<01:03,  5.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [00:55<00:55,  5.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [01:01<00:48,  5.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [01:07<00:44,  5.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [01:12<00:38,  5.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [01:18<00:34,  5.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [01:24<00:28,  5.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [01:29<00:22,  5.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [01:35<00:17,  5.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [01:40<00:11,  5.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [01:46<00:05,  5.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [01:52<00:00,  5.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training structural probe. This took 116.03203630447388s\n",
            "Layer 0 test UUAS: 0.6522645048608714\n",
            "Fitting probe...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-64150103750e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fitting probe...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mPOS_probe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPT_train_projected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Done fitting POS probe. This took {time.time() - t}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1292\u001b[0m             path_func(\n\u001b[1;32m   1293\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             ]\n\u001b[0;32m--> 450\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    451\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    694\u001b[0m                                  **options)\n\u001b[1;32m    695\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    697\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    698\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_linear_loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_pointwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_reg_strength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_pointwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0m\u001b[1;32m     47\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(f\"UUAS test scores list: {test_UUAS_per_layer}\")\n",
        "\n",
        "plt.scatter(range(1,8), test_UUAS_per_layer[1:])\n",
        "plt.xlabel(\"Layer in distilgpt2\")\n",
        "plt.ylabel(\"UUAS\")\n",
        "plt.savefig(\"Amnesic_UUAS_loss.png\")"
      ],
      "metadata": {
        "id": "D_sYO861YDA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baselines: layer UUAS score without amnesic probing**"
      ],
      "metadata": {
        "id": "54_yJgN8DM6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basline_test_UUAS_per_layer = []\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    print(f\"Examining layer {layer}...\")\n",
        "    t = time.time()\n",
        "    print(f\"Generating training data...\")\n",
        "    GPT_train_x, GPT_train_y, GPT_train_vocab = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
        "        model, \n",
        "        tokenizer,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating training data. This took {time.time() - t}s\")\n",
        "\n",
        "    t = time.time()\n",
        "    print(f\"Generating validation data...\")\n",
        "    GPT_dev_x, GPT_dev_y, _ = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
        "        model, \n",
        "        tokenizer,\n",
        "        pos_vocab=GPT_train_vocab,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating validation data. This took {time.time() - t}s\")\n",
        "\n",
        "    t = time.time()\n",
        "    print(f\"Generating test data...\")\n",
        "    GPT_test_x, GPT_test_y, _ = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
        "        model,\n",
        "        tokenizer,\n",
        "        pos_vocab=GPT_train_vocab,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating test data. This took {time.time() - t}s\")\n",
        "\n",
        "    t = time.time()\n",
        "    print(\"Training structural probe...\")\n",
        "    # Train the structural probe on these amnesic vectors\n",
        "    _, _, GPT_test_loss, GPT_test_uuas = train((gold_distances_train, GPT_train_x),\n",
        "                                                         (gold_distances_dev, GPT_dev_x),\n",
        "                                                         (gold_distances_test, GPT_test_x),\n",
        "                                                         device,\n",
        "                                                         epochs)\n",
        "    print(f\"Done training structural probe. This took {time.time() - t}s\")\n",
        "    print(f\"Baseline layer {layer} test UUAS: {GPT_test_uuas}\")\n",
        "    basline_test_UUAS_per_layer.append(GPT_test_uuas)\n",
        "\n",
        "    print(\"Deleting all data from memory...\")\n",
        "    del GPT_train_x\n",
        "    del GPT_train_y\n",
        "    del GPT_train_vocab\n",
        "    del GPT_dev_x\n",
        "    del GPT_dev_y\n",
        "    del GPT_test_x\n",
        "    del GPT_test_y\n",
        "    print(\"Done deleting data from memory. Moving on.\")"
      ],
      "metadata": {
        "id": "Am75UB-PDMOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Repeated amnesic probing**"
      ],
      "metadata": {
        "id": "HpCQRDnhJER9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# Only do repeated probing for last layer\n",
        "layer = 6\n",
        "num_repeats = 10\n",
        "\n",
        "print(f\"Examining layer {layer}...\")\n",
        "t = time.time()\n",
        "print(f\"Generating training data...\")\n",
        "GPT_train_x, GPT_train_y, GPT_train_vocab = create_data(\n",
        "    ud_parses,\n",
        "    os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
        "    model, \n",
        "    tokenizer,\n",
        "    layer=layer\n",
        ")\n",
        "print(f\"Done generating training data. This took {time.time() - t}s\")\n",
        "\n",
        "t = time.time()\n",
        "print(f\"Generating validation data...\")\n",
        "GPT_dev_x, GPT_dev_y, _ = create_data(\n",
        "    ud_parses,\n",
        "    os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
        "    model, \n",
        "    tokenizer,\n",
        "    pos_vocab=GPT_train_vocab,\n",
        "    layer=layer\n",
        ")\n",
        "print(f\"Done generating validation data. This took {time.time() - t}s\")\n",
        "\n",
        "t = time.time()\n",
        "print(f\"Generating test data...\")\n",
        "GPT_test_x, GPT_test_y, _ = create_data(\n",
        "    ud_parses,\n",
        "    os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
        "    model,\n",
        "    tokenizer,\n",
        "    pos_vocab=GPT_train_vocab,\n",
        "    layer=layer\n",
        ")\n",
        "print(f\"Done generating test data. This took {time.time() - t}s\")\n",
        "\n",
        "GPT_train_projected = copy.deepcopy(GPT_train_x)\n",
        "GPT_dev_projected = copy.deepcopy(GPT_dev_x)\n",
        "GPT_test_projected = copy.deepcopy(GPT_test_x)\n",
        "\n",
        "repeat_list = []\n",
        "for i in range(num_repeats):\n",
        "    POS_probe = LogisticRegression()\n",
        "    t = time.time()\n",
        "    print(f\"Fitting probe...\")\n",
        "    POS_probe.fit(torch.cat(GPT_train_projected, dim=0).cpu(), GPT_train_y)\n",
        "    print(f\"Done fitting POS probe. This took {time.time() - t}s\")\n",
        "\n",
        "    probe_weights = POS_probe.coef_\n",
        "\n",
        "    # Get the \"amnesic vectors\", the representations with POS information removed\n",
        "    GPT_train_projected = project_representations(GPT_train_projected, probe_weights)\n",
        "    GPT_dev_projected = project_representations(GPT_dev_projected, probe_weights)\n",
        "    GPT_test_projected = project_representations(GPT_test_projected, probe_weights)\n",
        "\n",
        "    t = time.time()\n",
        "    print(\"Training structural probe...\")\n",
        "    # Train the structural probe on these amnesic vectors\n",
        "    _, _, proj_GPT_test_loss, proj_GPT_test_uuas = train((gold_distances_train, GPT_train_projected),\n",
        "                                                        (gold_distances_dev, GPT_dev_projected),\n",
        "                                                        (gold_distances_test, GPT_test_projected),\n",
        "                                                        device,\n",
        "                                                        epochs)\n",
        "    print(f\"Done training structural probe. This took {time.time() - t}s\")\n",
        "    print(f\"Layer {layer} test UUAS: {proj_GPT_test_uuas}\")\n",
        "    repeat_list.append(proj_GPT_test_uuas)\n",
        "\n",
        "print(\"Deleting all data from memory...\")\n",
        "del GPT_train_x\n",
        "del GPT_train_y\n",
        "del GPT_train_vocab\n",
        "del GPT_dev_x\n",
        "del GPT_dev_y\n",
        "del GPT_test_x\n",
        "del GPT_test_y\n",
        "del POS_probe\n",
        "del GPT_train_projected\n",
        "del GPT_dev_projected\n",
        "del GPT_test_projected\n",
        "print(\"Done deleting data from memory. Moving on.\")\n",
        "\n",
        "print(repeat_list)"
      ],
      "metadata": {
        "id": "-kMI_p58JNtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuUfLkgzW8Kv"
      },
      "source": [
        "**Debug cells**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MaXSWw0W8Kv"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "\n",
        "# num_layers = 7\n",
        "# epochs = 20\n",
        "# test_UUAS_per_layer = []\n",
        "# use_sample = True\n",
        "# model_name = \"GPT\"\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# # conllu_path = \"/content/drive/My Drive/nlp2-probing-lms/data/\"\n",
        "# conllu_path = \"data/\"\n",
        "\n",
        "# if use_sample:\n",
        "#   conllu_path += \"sample/\"\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating training distances...\")\n",
        "# gold_distances_train = get_struct_data(f\"{conllu_path}en_ewt-ud-train.conllu\")\n",
        "# print(f\"Done generating training distances. This took {time.time() - t}s\")\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating validation distances...\")\n",
        "# gold_distances_dev = get_struct_data(f\"{conllu_path}en_ewt-ud-dev.conllu\")\n",
        "# print(f\"Done generating validation distances. This took {time.time() - t}s\")\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating testing distances...\")\n",
        "# gold_distances_test = get_struct_data(f\"{conllu_path}en_ewt-ud-test.conllu\")\n",
        "# print(f\"Done generating testing distances. This took {time.time() - t}s\")\n",
        "\n",
        "# layer = 1\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating training data...\")\n",
        "# GPT_train_x, GPT_train_y, GPT_train_vocab = create_data(\n",
        "#     ud_parses,\n",
        "#     os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
        "#     model, \n",
        "#     tokenizer,\n",
        "#     layer=layer\n",
        "# )\n",
        "# print(f\"Done generating training data. This took {time.time() - t}s\")\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating validation data...\")\n",
        "# GPT_dev_x, GPT_dev_y, _ = create_data(\n",
        "#     ud_parses,\n",
        "#     os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
        "#     model, \n",
        "#     tokenizer,\n",
        "#     pos_vocab=GPT_train_vocab,\n",
        "#     layer=layer\n",
        "# )\n",
        "# print(f\"Done generating validation data. This took {time.time() - t}s\")\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating test data...\")\n",
        "# GPT_test_x, GPT_test_y, _ = create_data(\n",
        "#     ud_parses,\n",
        "#     os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
        "#     model,\n",
        "#     tokenizer,\n",
        "#     pos_vocab=GPT_train_vocab,\n",
        "#     layer=layer\n",
        "# )\n",
        "# print(f\"Done generating test data. This took {time.time() - t}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOpDBiagW8Kv"
      },
      "outputs": [],
      "source": [
        "# POS_probe = LogisticRegression()\n",
        "# t = time.time()\n",
        "# print(f\"Fitting probe...\")\n",
        "\n",
        "# # print(len(GPT_train_x[2][0]))\n",
        "\n",
        "# # print(torch.cat(GPT_train_x, dim=0).size())\n",
        "# # print(torch.tensor(GPT_train_x).size())\n",
        "\n",
        "# POS_probe.fit(torch.cat(GPT_train_x, dim=0), GPT_train_y)\n",
        "# print(f\"Done fitting POS probe. This took {time.time() - t}s\")\n",
        "\n",
        "# # Predict using the trained model\n",
        "# # GPT_train_predictions = POS_probe.predict(GPT_train_x)\n",
        "# # GPT_dev_predictions = POS_probe.predict(GPT_dev_x)\n",
        "# # GPT_test_predictions = POS_probe.predict(GPT_test_x)\n",
        "\n",
        "# # # Calculate accuracy scores\n",
        "# # GPT_train_accuracy = accuracy_score(GPT_train_y, GPT_train_predictions)\n",
        "# # GPT_dev_accuracy = accuracy_score(GPT_dev_y, GPT_dev_predictions)\n",
        "# # GPT_test_accuracy = accuracy_score(GPT_test_y, GPT_test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVWwBTbaW8Kw"
      },
      "outputs": [],
      "source": [
        "# # print(GPT_train_x.shape())\n",
        "# # print(GPT_train_projected.shape())\n",
        "\n",
        "# probe_weights = POS_probe.coef_\n",
        "\n",
        "# # Get the \"amnesic vectors\", the representations with POS information removed\n",
        "# GPT_train_projected = project_representations(GPT_train_x, probe_weights)\n",
        "# GPT_dev_projected = project_representations(GPT_dev_x, probe_weights)\n",
        "# GPT_test_projected = project_representations(GPT_test_x, probe_weights)\n",
        "\n",
        "# t = time.time()\n",
        "# print(\"Training structural probe...\")\n",
        "\n",
        "# # Train the structural probe on these amnesic vectors\n",
        "# _, _, proj_GPT_test_loss, proj_GPT_test_uuas = train((gold_distances_train, GPT_train_projected),\n",
        "#                                                         (gold_distances_dev, GPT_dev_projected),\n",
        "#                                                         (gold_distances_test, GPT_test_projected),\n",
        "#                                                         device,\n",
        "#                                                         epochs)\n",
        "# print(f\"Done training structural probe. This took {time.time() - t}s\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f23bc515b0574a1ab59893f28bf00da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f54acfa0adbb476a95b4c5205165f589",
              "IPY_MODEL_cbdb9ce9f3044615986869f7ea2bb4ec",
              "IPY_MODEL_5617402ea18a459e852ea61d9bbea8a3"
            ],
            "layout": "IPY_MODEL_767ac8b7dbaa492eb0251f10310ed374"
          }
        },
        "f54acfa0adbb476a95b4c5205165f589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b01663fe49b04eaab75d9b8904d91ce0",
            "placeholder": "​",
            "style": "IPY_MODEL_e88da32b93c04c2c833eee14bd93b475",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "cbdb9ce9f3044615986869f7ea2bb4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb1a60c920f84539bc322f2b4205303b",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edcc7b1ab38e479a96dd167137acb75b",
            "value": 1042301
          }
        },
        "5617402ea18a459e852ea61d9bbea8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68c7510199364886a3c67daacc5f8ebf",
            "placeholder": "​",
            "style": "IPY_MODEL_482803b02786410cb05131744eb057e4",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 5.44MB/s]"
          }
        },
        "767ac8b7dbaa492eb0251f10310ed374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b01663fe49b04eaab75d9b8904d91ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88da32b93c04c2c833eee14bd93b475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb1a60c920f84539bc322f2b4205303b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edcc7b1ab38e479a96dd167137acb75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68c7510199364886a3c67daacc5f8ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "482803b02786410cb05131744eb057e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30db659aef8a443395f2c2809dabc0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94439f8e4023486fa8467c2992693c1b",
              "IPY_MODEL_814dc358cad04919a12305e9c5e65fe3",
              "IPY_MODEL_aa2597aa60614a2c9105c699ebc89abd"
            ],
            "layout": "IPY_MODEL_bc1a110f5a2d45c8b2b416e1aad0b741"
          }
        },
        "94439f8e4023486fa8467c2992693c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_646022388cce424d9ffe8057468fea85",
            "placeholder": "​",
            "style": "IPY_MODEL_4b3cb13b2d9c4f198e8db5f764f9a2b8",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "814dc358cad04919a12305e9c5e65fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3dabfff875349ccb1a26fb2af68c720",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6a57c969fac4cc99bbd52fde33c7501",
            "value": 456318
          }
        },
        "aa2597aa60614a2c9105c699ebc89abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c45284ca198543f6b13a580a2c7aba36",
            "placeholder": "​",
            "style": "IPY_MODEL_0f62a5469f5e476697e7ff0b94fb2707",
            "value": " 456k/456k [00:00&lt;00:00, 6.22MB/s]"
          }
        },
        "bc1a110f5a2d45c8b2b416e1aad0b741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "646022388cce424d9ffe8057468fea85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3cb13b2d9c4f198e8db5f764f9a2b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3dabfff875349ccb1a26fb2af68c720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a57c969fac4cc99bbd52fde33c7501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c45284ca198543f6b13a580a2c7aba36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f62a5469f5e476697e7ff0b94fb2707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e07bbd681b44941858dd0e379a8e5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad5ec7bd878d40518b2123e1692da520",
              "IPY_MODEL_39552553223640e28fd3544c1e58a8a7",
              "IPY_MODEL_1af3482119f841ddab8075319ae892a1"
            ],
            "layout": "IPY_MODEL_d56ffcd56a6b4ea0a4d67c4af45861e7"
          }
        },
        "ad5ec7bd878d40518b2123e1692da520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ae9b29d19f7471aba621f9028607816",
            "placeholder": "​",
            "style": "IPY_MODEL_d713988d66e04e9b94ce69f9d48a53b0",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "39552553223640e28fd3544c1e58a8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc83b69c41684fb7997afd0eb4f8b00d",
            "max": 762,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45a32f64d9454bbe88b37c59c753cb69",
            "value": 762
          }
        },
        "1af3482119f841ddab8075319ae892a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7d604377e82478789678096b4455772",
            "placeholder": "​",
            "style": "IPY_MODEL_55478e0c8c7340498faf6f376d1e5d1a",
            "value": " 762/762 [00:00&lt;00:00, 25.9kB/s]"
          }
        },
        "d56ffcd56a6b4ea0a4d67c4af45861e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ae9b29d19f7471aba621f9028607816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d713988d66e04e9b94ce69f9d48a53b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc83b69c41684fb7997afd0eb4f8b00d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a32f64d9454bbe88b37c59c753cb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7d604377e82478789678096b4455772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55478e0c8c7340498faf6f376d1e5d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5c3f39cc8464ba891a69d27d65b2071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b410afb7d9f3486ca83a7128a0e9b895",
              "IPY_MODEL_c2ca9daa63f64e3c9da19876aabed539",
              "IPY_MODEL_cc82abd08eed4474b2d7cee8d7c893b1"
            ],
            "layout": "IPY_MODEL_357c1c325e0f426f91e4bf74bb98f444"
          }
        },
        "b410afb7d9f3486ca83a7128a0e9b895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2c35319cd4344de8369f86de469371d",
            "placeholder": "​",
            "style": "IPY_MODEL_ff05731df373484b990fe820141200c2",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "c2ca9daa63f64e3c9da19876aabed539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a27ebe4bae8420ab2edc298e22983dc",
            "max": 352833716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83f600c1d76a43258479c5a52050b6cd",
            "value": 352833716
          }
        },
        "cc82abd08eed4474b2d7cee8d7c893b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29f11ceb2ca64647b1b8ff27f2ef55d0",
            "placeholder": "​",
            "style": "IPY_MODEL_50cf7af0182f45bfabffbac0faf72f81",
            "value": " 353M/353M [00:06&lt;00:00, 57.1MB/s]"
          }
        },
        "357c1c325e0f426f91e4bf74bb98f444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2c35319cd4344de8369f86de469371d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff05731df373484b990fe820141200c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a27ebe4bae8420ab2edc298e22983dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f600c1d76a43258479c5a52050b6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29f11ceb2ca64647b1b8ff27f2ef55d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50cf7af0182f45bfabffbac0faf72f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b29592f2b2249e8a6e732828c1b4317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bee1cc3f5344e0ba0b7c74a5a8cf1cb",
              "IPY_MODEL_ca29dc72e7b349d9bc14c7d2c5c75333",
              "IPY_MODEL_85d2d3832fb34e95a2c0467a2601eab1"
            ],
            "layout": "IPY_MODEL_69b3f80276eb4eff954fd67ee7e91a19"
          }
        },
        "7bee1cc3f5344e0ba0b7c74a5a8cf1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a65041f9287d4ba4adce798431582d0c",
            "placeholder": "​",
            "style": "IPY_MODEL_9b4c718eab104583ab2f0d68e92434db",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "ca29dc72e7b349d9bc14c7d2c5c75333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41ce4de6ed504f5da174d398268c5a26",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71d8256ed8594dfb8ad7ea33160dc2a4",
            "value": 124
          }
        },
        "85d2d3832fb34e95a2c0467a2601eab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fe8a3a48867490293312f4fd81e9e81",
            "placeholder": "​",
            "style": "IPY_MODEL_c92cee4c2b1e40e09cc828673056fddd",
            "value": " 124/124 [00:00&lt;00:00, 7.61kB/s]"
          }
        },
        "69b3f80276eb4eff954fd67ee7e91a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a65041f9287d4ba4adce798431582d0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b4c718eab104583ab2f0d68e92434db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41ce4de6ed504f5da174d398268c5a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71d8256ed8594dfb8ad7ea33160dc2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fe8a3a48867490293312f4fd81e9e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c92cee4c2b1e40e09cc828673056fddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}