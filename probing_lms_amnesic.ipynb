{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9IsZIGEwCM"
      },
      "source": [
        "Google Colab stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V7o8UHIEwCO",
        "outputId": "a3bd74d6-4735-4e6f-b25a-6949546163cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (4.5.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install dependencies\n",
        "%pip install transformers\n",
        "%pip install pytorch\n",
        "%pip install scikit-learn\n",
        "%pip install conllu\n",
        "%pip install tqdm\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/nlp2-probing-lms/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "53c6e7fe69c14742a5d9e5dddaca6ad1",
            "2d3927d68fde43e8b1a1714dcbfc6d70",
            "b5203c4c023644e5af4d2fb583ed8ab7",
            "609fe980bf6e4a0193e8b8762b97e2ea",
            "89a0aceec46445fcac15cad80c25143a",
            "3412d6e0a822403e9eced42a27be5e83",
            "b93fb7c0f94f4e87b10a9f1f4cce1f72",
            "c6a194c45c754640820dc951dac1aabe",
            "f3625843bf7c47bbb9851a72358063fb",
            "bcf097747c2049fb8405bc2b73a72a78",
            "095d6aa08abe442a867cd3149465c85c",
            "b46fc293aee74cdbbcbc05659a463e48",
            "678e392ed6954fe2b8fc2c5f18697c0a",
            "1a7c07d906cb495193c2563cb74f0b93",
            "c74447bd75894fd99e02d51fd3251910",
            "573333e7d39e4ac1abe6268ab58a9eef",
            "70cc5fde5a9a4564936d1678630b31e5",
            "1c6998ba327b406d81d4fde7c53084c1",
            "1c2fbe4f99414ce08a014e3edac25f66",
            "c28397a9e410436099f9e854f3953f26",
            "411cbda37a5b4b97a296523d5aee68ee",
            "41706e356ea54429ae9bed2ea07cb985",
            "50a87e2d410a4646aac69e6ec569fe45",
            "ce49711278fc4c2d8e3bbcd0080b4ee8",
            "0efc75fc035a481db06dffb4c67ffe5c",
            "79ef487a2c634720a5b5e224873f2124",
            "63965dde784144f3b26f207702df4c5f",
            "12d249faa23d4210adcdd33ef19c0d68",
            "26e3c86bc6424d9d82b57e98de567412",
            "9684e6be3e11412f956bfb042da9b586",
            "e9c9f81639244160a5d6b2b03ae7af10",
            "d4f270d070b94337857d4dac32dc0917",
            "dee330099c9943399078d7e7079c501a",
            "60f47f8819a34b2eb59eecc6e030d9f4",
            "8ddb3327a9f94b5998d7e090683c57d6",
            "6f5fcfcc935444acb984c8efd1bb079c",
            "12e3aa25538c4be49e659a473b06996a",
            "b5434fd6ed81433dbbea72201f2a2278",
            "d0dbf077edd145708b5eccd1cb27ac6c",
            "a7f1e0b0fc9c47da9fd0cdade0ee270e",
            "dbd7bac5d4e748818521de315047aaa0",
            "4ba945dd773f403da285123f401bd956",
            "61a8246918c242ebaed044594ddc2909",
            "b155fb2de4d548c5bf603eea303195a5",
            "239d8ce4aefb4b8b9607bed1f551932b",
            "d0018b1d2645410781e306b8b5e0ba85",
            "0054a95d49324d98965135558d69f4af",
            "97fa77a9a3d246bfb2869d853941ee0c",
            "eba3f378e34e4cc2b18a85a86878b2f8",
            "16b26704213c4bedaeea101cd4055423",
            "9e5a221a72024d1f9241d7dadff8a9a6",
            "d6835cea3a77456684bc55c7ba1ee83d",
            "a6674badd6934fe6a5ffd5b71e8e5500",
            "2d6d617f980a4be890f4530579e394f3",
            "68e4a1838fc44b1fb3332372957bb3b4"
          ]
        },
        "id": "1yxaD-0rEwCP",
        "outputId": "a4379e2e-1333-4384-98e3-2805cf7cc69f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "loading file vocab.json from cache at C:\\Users\\Cyproxius/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\38cc92ec43315abd5136313225e95acc5986876c\\vocab.json\n",
            "loading file merges.txt from cache at C:\\Users\\Cyproxius/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\38cc92ec43315abd5136313225e95acc5986876c\\merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at C:\\Users\\Cyproxius/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\38cc92ec43315abd5136313225e95acc5986876c\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at C:\\Users\\Cyproxius/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\38cc92ec43315abd5136313225e95acc5986876c\\config.json\n",
            "Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at C:\\Users\\Cyproxius/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\38cc92ec43315abd5136313225e95acc5986876c\\pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at C:\\Users\\Cyproxius/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\38cc92ec43315abd5136313225e95acc5986876c\\generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.29.2\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "## Your code for initializing the transformer model(s)\n",
        "#\n",
        "# Note that transformer models use their own `tokenizer`, that should be loaded in as well.\n",
        "#\n",
        "from transformers import *\n",
        "import transformers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('distilgpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aXBxC6ihEwCQ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "## Your code for initializing the rnn model(s)\n",
        "#\n",
        "# The Gulordava LSTM model can be found here: \n",
        "# https://drive.google.com/file/d/19Lp3AM4NEPycp_IBgoHfLc_V456pmUom/view?usp=sharing\n",
        "# You can read more about this model in the original paper here: https://arxiv.org/pdf/1803.11138.pdf\n",
        "#\n",
        "# N.B: I have altered the RNNModel code to only output the hidden states that you are interested in.\n",
        "# If you want to do more experiments with this model you could have a look at the original code here:\n",
        "# https://github.com/facebookresearch/colorlessgreenRNNs/blob/master/src/language_models/model.py\n",
        "#\n",
        "from collections import defaultdict\n",
        "from lstm.model import RNNModel\n",
        "import torch\n",
        "\n",
        "# pre_path = \"/content/drive/My Drive/nlp2-probing-lms/\"\n",
        "pre_path = \"\"\n",
        "\n",
        "model_location = f'{pre_path}state_dictLSTM.pt'  # <- point this to the location of the Gulordava .pt file\n",
        "lstm = RNNModel('LSTM', 50001, 650, 650, 2)\n",
        "lstm.load_state_dict(torch.load(model_location))\n",
        "\n",
        "\n",
        "# This LSTM does not use a Tokenizer like the Transformers, but a Vocab dictionary that maps a token to an id.\n",
        "with open(f'{pre_path}lstm/vocab.txt', encoding=\"utf8\") as f:\n",
        "    w2i = {w.strip(): i for i, w in enumerate(f)}\n",
        "\n",
        "vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
        "vocab.update(w2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NNAWS0P-EwCR"
      },
      "outputs": [],
      "source": [
        "# READ DATA\n",
        "from typing import List\n",
        "from conllu import parse_incr, TokenList\n",
        "\n",
        "\n",
        "# If stuff like `: str` and `-> ..` seems scary, fear not! \n",
        "# These are type hints that help you to understand what kind of argument and output is expected.\n",
        "def parse_corpus(filename: str) -> List[TokenList]:\n",
        "    data_file = open(filename, encoding=\"utf-8\")\n",
        "\n",
        "    ud_parses = list(parse_incr(data_file))\n",
        "    \n",
        "    return ud_parses\n",
        "\n",
        "\n",
        "ud_parses = parse_corpus(f'{pre_path}data/sample/en_ewt-ud-train.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xPikUzvAEwCR"
      },
      "outputs": [],
      "source": [
        "# FETCH SENTENCE REPRESENTATIONS\n",
        "from torch import Tensor\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Should return a tensor of shape (num_tokens_in_corpus, representation_size)\n",
        "# Make sure you correctly average the subword representations that belong to 1 token!\n",
        "\n",
        "def fetch_sen_reps(ud_parses: List[TokenList], model, tokenizer, concat=True, layer=-1) -> Tensor:\n",
        "    \n",
        "    model.eval()\n",
        "    # Initialize an empty list to store sentence representations\n",
        "    corpus_rep = []\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Transformer model\n",
        "    if isinstance(tokenizer, transformers.PreTrainedTokenizer):\n",
        "        # Loop through the parses\n",
        "        for parse in tqdm(ud_parses):\n",
        "            prev_token = None\n",
        "            encoded_full = []\n",
        "            encoded = []\n",
        "            tokens = []\n",
        "            # Loop through the tokens in the parse\n",
        "            for token in parse:\n",
        "                # Check if the previous token has a misc attribute and SpaceAfter key\n",
        "                if prev_token is None or ('misc' in prev_token and prev_token['misc'] is not None and 'SpaceAfter' in prev_token['misc'] and prev_token['misc']['SpaceAfter'] == 'No'):\n",
        "                    tokens.append(token['form'])\n",
        "                else:\n",
        "                    tokens.append(' ' + token['form'])\n",
        "                prev_token = token\n",
        "\n",
        "            # Encode each word using the tokenizer\n",
        "            for token in tokens:\n",
        "                with torch.no_grad():\n",
        "                    encoded_token = tokenizer.encode(token, add_special_tokens=False)\n",
        "                    encoded.extend(encoded_token)\n",
        "                    encoded_full.append(encoded_token)\n",
        "\n",
        "            # Convert the encoded subword to a tensor        \n",
        "            input_ids = torch.tensor(encoded).unsqueeze(0)\n",
        "            input_ids = input_ids.to(device)\n",
        "            # Get the sentence representation from the model's output\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, output_hidden_states=True)\n",
        "\n",
        "            last_layer_hidden_state = outputs.hidden_states[layer]\n",
        "            sen_rep = last_layer_hidden_state.squeeze()\n",
        "            # Fix for one token sentences\n",
        "            if len(parse) <= 1:\n",
        "                sen_rep = sen_rep.unsqueeze(0)\n",
        "            # Average the representations of subwords\n",
        "            sen_rep_avg = []\n",
        "            i = 0\n",
        "            for word in encoded_full:\n",
        "                j = len(word)\n",
        "                sen_rep_avg.append(torch.mean(sen_rep[i:i+j], dim=0, keepdim=True))\n",
        "                i += j\n",
        "            tensor = torch.stack(sen_rep_avg, dim=0).squeeze(1)\n",
        "            if tensor.shape[0] == 1:\n",
        "                tensor = tensor.view(1, -1, tensor.shape[-1])[:, 0, :]      \n",
        "            corpus_rep.append(tensor)\n",
        "        \n",
        "    # LSTM model\n",
        "    else:\n",
        "        for parse in ud_parses:\n",
        "            encoded = [tokenizer.get(token['form'].replace(' ', ''), tokenizer['<unk>']) for token in parse]\n",
        "            input_ids = torch.tensor(encoded).unsqueeze(0)\n",
        "            input_ids = input_ids.to(device)\n",
        "            hidden = model.init_hidden(1)\n",
        "            with torch.no_grad():\n",
        "                tensor = lstm(input_ids, hidden).squeeze()\n",
        "            if len(tensor.shape) == 1:\n",
        "                tensor = tensor.unsqueeze(0)\n",
        "            corpus_rep.append(tensor)\n",
        "    if concat:\n",
        "        corpus_rep = torch.cat(corpus_rep, dim=0)\n",
        "    \n",
        "    return corpus_rep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rKOGY--7EwCS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n"
          ]
        }
      ],
      "source": [
        "def error_msg(model_name, gold_embs, embs, i2w):\n",
        "    with open(f'{pre_path}{model_name}_tokens1.pickle', 'rb') as f:\n",
        "        sen_tokens = pickle.load(f)\n",
        "        \n",
        "    diff = torch.abs(embs - gold_embs)\n",
        "    max_diff = torch.max(diff)\n",
        "    avg_diff = torch.mean(diff)\n",
        "    \n",
        "    print(f\"{model_name} embeddings don't match!\")\n",
        "    print(f\"Max diff.: {max_diff:.4f}\\nMean diff. {avg_diff:.4f}\")\n",
        "\n",
        "    print(\"\\nCheck if your tokenization matches with the original tokenization:\")\n",
        "    for idx in sen_tokens.squeeze():\n",
        "        if isinstance(i2w, list):\n",
        "            token = i2w[idx]\n",
        "        else:\n",
        "            token = i2w.convert_ids_to_tokens(idx.item())\n",
        "        print(f\"{idx:<6} {token}\")\n",
        "\n",
        "\n",
        "def assert_sen_reps(model, tokenizer, lstm, vocab):\n",
        "    with open(f'{pre_path}distilgpt2_emb1.pickle', 'rb') as f:\n",
        "        distilgpt2_emb1 = pickle.load(f)\n",
        "        \n",
        "    with open(f'{pre_path}lstm_emb1.pickle', 'rb') as f:\n",
        "        lstm_emb1 = pickle.load(f)\n",
        "    \n",
        "    corpus = parse_corpus(f'{pre_path}data/sample/en_ewt-ud-train.conllu')[:1]\n",
        "    \n",
        "    own_distilgpt2_emb1 = fetch_sen_reps(corpus, model, tokenizer)\n",
        "    own_lstm_emb1 = fetch_sen_reps(corpus, lstm, vocab)\n",
        "    \n",
        "    # assert distilgpt2_emb1.shape == own_distilgpt2_emb1.shape, \\\n",
        "    #     f\"Distilgpt2 shape mismatch: {distilgpt2_emb1.shape} (gold) vs. {own_distilgpt2_emb1.shape} (yours)\"\n",
        "    # assert lstm_emb1.shape == own_lstm_emb1.shape, \\\n",
        "    #     f\"LSTM shape mismatch: {lstm_emb1.shape} (gold) vs. {own_lstm_emb1.shape} (yours)\"\n",
        "\n",
        "    # if not torch.allclose(distilgpt2_emb1, own_distilgpt2_emb1, rtol=1e-3, atol=1e-3):\n",
        "    #     error_msg(\"distilgpt2\", distilgpt2_emb1, own_distilgpt2_emb1, tokenizer)\n",
        "    # if not torch.allclose(lstm_emb1, own_lstm_emb1, rtol=1e-3, atol=1e-3):\n",
        "    #     error_msg(\"lstm\", lstm_emb1, own_lstm_emb1, list(vocab.keys()))\n",
        "\n",
        "\n",
        "assert_sen_reps(model, tokenizer, lstm, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_NTSvmLEwCS",
        "outputId": "dbac7e87-da05-486c-fe9a-d2018e58cb33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0, 1, 0,  ..., 3, 0, 4]),\n",
              " {'PROPN': 0,\n",
              "  'PUNCT': 1,\n",
              "  'ADJ': 2,\n",
              "  'NOUN': 3,\n",
              "  'VERB': 4,\n",
              "  'DET': 5,\n",
              "  'ADP': 6,\n",
              "  'AUX': 7,\n",
              "  'PRON': 8,\n",
              "  'PART': 9,\n",
              "  'SCONJ': 10,\n",
              "  'NUM': 11,\n",
              "  'ADV': 12,\n",
              "  'CCONJ': 13})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# FETCH POS LABELS\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "# Should return a tensor of shape (num_tokens_in_corpus,)\n",
        "# Make sure that when fetching these pos tags for your train/dev/test corpora you share the label vocabulary.\n",
        "def fetch_pos_tags(ud_parses: List[TokenList], pos_vocab: Optional[Dict[str, int]] = None) -> Tuple[Tensor, Dict[str, int]]:\n",
        "    \n",
        "    # Create label dictionary if not passed to function\n",
        "    if pos_vocab is None:\n",
        "        pos_vocab = {}\n",
        "        pos_counter = 0\n",
        "    else:\n",
        "        pos_counter = len(pos_vocab.values())\n",
        "\n",
        "    # Keep a list of all POS tag labels\n",
        "    all_tags = []\n",
        "\n",
        "    for parse in ud_parses:\n",
        "        for token in parse:\n",
        "            # Extract the POS tag for the given token\n",
        "            pos_tag = token['upostag']\n",
        "            \n",
        "            # If the token is not yet in the vocabulary, add it\n",
        "            if pos_tag not in pos_vocab.keys():\n",
        "                pos_vocab[pos_tag] = pos_counter\n",
        "                pos_counter += 1\n",
        "            \n",
        "            # Add the label encoding of the POS tag\n",
        "            all_tags.append(pos_vocab[pos_tag])\n",
        "\n",
        "    return torch.tensor(all_tags), pos_vocab\n",
        "\n",
        "\n",
        "fetch_pos_tags(ud_parses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "lzc-4t7XEwCT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Function that combines the previous functions, and creates 2 tensors for a .conllu file: \n",
        "# 1 containing the token representations, and 1 containing the (tokenized) pos_tags.\n",
        "\n",
        "def create_data(ud_parses, filename: str, lm, w2i, pos_vocab=None, layer=-1):\n",
        "    new_parses = parse_corpus(filename)\n",
        "    \n",
        "    sen_reps = fetch_sen_reps(new_parses, lm, w2i, concat=False, layer=layer)\n",
        "    pos_tags, pos_vocab = fetch_pos_tags(new_parses, pos_vocab=pos_vocab)\n",
        "    \n",
        "    return sen_reps, pos_tags, pos_vocab"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nwgsTqqyEwCT"
      },
      "source": [
        "# Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WBz0d2zpEwCT"
      },
      "outputs": [],
      "source": [
        "# In case you want to transform your conllu tree to an nltk.Tree, for better visualisation\n",
        "\n",
        "def rec_tokentree_to_nltk(tokentree):\n",
        "    token = tokentree.token[\"form\"]\n",
        "    tree_str = f\"({token} {' '.join(rec_tokentree_to_nltk(t) for t in tokentree.children)})\"\n",
        "\n",
        "    return tree_str\n",
        "\n",
        "\n",
        "def tokentree_to_nltk(tokentree):\n",
        "    from nltk import Tree as NLTKTree\n",
        "\n",
        "    tree_str = rec_tokentree_to_nltk(tokentree)\n",
        "\n",
        "    return NLTKTree.fromstring(tree_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7WFVgNxEwCT",
        "outputId": "1b808c00-29d5-47d6-83a8-e8afee4ac7bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ete3\n",
            "  Using cached ete3-3.1.3.tar.gz (4.8 MB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: ete3\n",
            "  Building wheel for ete3 (setup.py): started\n",
            "  Building wheel for ete3 (setup.py): finished with status 'done'\n",
            "  Created wheel for ete3: filename=ete3-3.1.3-py3-none-any.whl size=2273809 sha256=f4e40aa3763955f96acf2d8c6e4fb3fbdd914f877f750a736b8ac39c3be87c36\n",
            "  Stored in directory: c:\\users\\cyproxius\\appdata\\local\\pip\\cache\\wheels\\a0\\72\\00\\1982bd848e52b03079dbf800900120bc1c20e92e9a1216e525\n",
            "Successfully built ete3\n",
            "Installing collected packages: ete3\n",
            "Successfully installed ete3-3.1.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install ete3\n",
        "from ete3 import Tree as EteTree\n",
        "\n",
        "\n",
        "class FancyTree(EteTree):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, format=1, **kwargs)\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.get_ascii(show_internal=True)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "\n",
        "def rec_tokentree_to_ete(tokentree):\n",
        "    idx = str(tokentree.token[\"id\"])\n",
        "    children = tokentree.children\n",
        "    if children:\n",
        "        return f\"({','.join(rec_tokentree_to_ete(t) for t in children)}){idx}\"\n",
        "    else:\n",
        "        return idx\n",
        "    \n",
        "def tokentree_to_ete(tokentree):\n",
        "    newick_str = rec_tokentree_to_ete(tokentree)\n",
        "\n",
        "    return FancyTree(f\"{newick_str};\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jcnzC7eEwCU",
        "outputId": "84d51ca2-5846-4c21-959a-bcacd31f1e85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   /-2\n",
            "  |\n",
            "  |--3\n",
            "  |\n",
            "  |--4\n",
            "  |\n",
            "  |   /6 /-5\n",
            "  |  |\n",
            "  |  |   /-9\n",
            "  |  |  |\n",
            "  |  |  |--10\n",
            "  |  |  |\n",
            "  |  |  |--11\n",
            "  |  |-8|\n",
            "  |  |  |--12\n",
            "  |-7|  |\n",
            "  |  |  |--13\n",
            "  |  |  |\n",
            "  |  |   \\15/-14\n",
            "-1|  |\n",
            "  |  |   /-16\n",
            "  |  |  |\n",
            "  |  |  |--17\n",
            "  |  |  |\n",
            "  |   \\18   /-19\n",
            "  |     |  |\n",
            "  |     |  |--20\n",
            "  |     |  |\n",
            "  |     |  |-23/-22\n",
            "  |      \\21\n",
            "  |        |--24\n",
            "  |        |\n",
            "  |        |   /-25\n",
            "  |        |  |\n",
            "  |         \\28--26\n",
            "  |           |\n",
            "  |            \\-27\n",
            "  |\n",
            "   \\-29\n"
          ]
        }
      ],
      "source": [
        "# Let's check if it works!\n",
        "# We can read in a corpus using the code that was already provided, and convert it to an ete3 Tree.\n",
        "\n",
        "def parse_corpus(filename):\n",
        "    from conllu import parse_incr\n",
        "\n",
        "    data_file = open(filename, encoding=\"utf-8\")\n",
        "\n",
        "    ud_parses = list(parse_incr(data_file))\n",
        "    \n",
        "    return ud_parses\n",
        "\n",
        "corpus = parse_corpus(f'{pre_path}data/sample/en_ewt-ud-train.conllu')\n",
        "item = corpus[0]\n",
        "tokentree = item.to_tree()\n",
        "ete3_tree = tokentree_to_ete(tokentree)\n",
        "print(ete3_tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vu2ms3V4EwCU"
      },
      "outputs": [],
      "source": [
        "def create_gold_distances(corpus):\n",
        "    all_distances = []\n",
        "\n",
        "    for item in corpus:\n",
        "        tokentree = item.to_tree()\n",
        "        ete_tree = tokentree_to_ete(tokentree)\n",
        "\n",
        "        sen_len = len(ete_tree.search_nodes())\n",
        "        distances = torch.zeros((sen_len, sen_len))\n",
        "\n",
        "        # Compute all the distances\n",
        "        for i in range(sen_len):\n",
        "            for j in range(sen_len):\n",
        "                if i == j:\n",
        "                    distances[i,j] = 0.0\n",
        "                    distances[j,i] = 0.0\n",
        "                else:\n",
        "                    distances[i,j] = ete_tree.get_distance(str(i+1), str(j+1))\n",
        "                    distances[j,i] = ete_tree.get_distance(str(i+1), str(j+1))\n",
        "\n",
        "        all_distances.append(distances)\n",
        "\n",
        "    return all_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-nM3bqELEwCU"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "import torch\n",
        "\n",
        "\n",
        "def create_mst(distances):\n",
        "    distances = torch.triu(distances).detach().numpy()\n",
        "    mst = minimum_spanning_tree(distances).toarray()\n",
        "    mst[mst>0] = 1.\n",
        "    \n",
        "    return mst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIgC-rmtEwCU",
        "outputId": "7302e094-0c20-4fb7-ec5b-b77c5c6a23e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   /2 /-1\n",
            "  |\n",
            "  |--3\n",
            "  |\n",
            "  |--4\n",
            "  |\n",
            "  |   /-6\n",
            "  |  |\n",
            "-5|  |--7\n",
            "  |-8|\n",
            "  |  |   /-9\n",
            "  |  |  |\n",
            "  |   \\12--10\n",
            "  |     |\n",
            "  |      \\-11\n",
            "  |\n",
            "   \\-13 \n",
            "\n",
            "tensor([[0., 1., 3., 3., 2., 4., 4., 3., 5., 5., 5., 4., 3.],\n",
            "        [1., 0., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [3., 2., 0., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [3., 2., 2., 0., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [2., 1., 1., 1., 0., 2., 2., 1., 3., 3., 3., 2., 1.],\n",
            "        [4., 3., 3., 3., 2., 0., 2., 1., 3., 3., 3., 2., 3.],\n",
            "        [4., 3., 3., 3., 2., 2., 0., 1., 3., 3., 3., 2., 3.],\n",
            "        [3., 2., 2., 2., 1., 1., 1., 0., 2., 2., 2., 1., 2.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 0., 2., 2., 1., 4.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 0., 2., 1., 4.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 2., 0., 1., 4.],\n",
            "        [4., 3., 3., 3., 2., 2., 2., 1., 1., 1., 1., 0., 3.],\n",
            "        [3., 2., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 0.]]) \n",
            "\n",
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([[0., 1., 3., 3., 2., 4., 4., 3., 5., 5., 5., 4., 3.],\n",
            "        [1., 0., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [3., 2., 0., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [3., 2., 2., 0., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
            "        [2., 1., 1., 1., 0., 2., 2., 1., 3., 3., 3., 2., 1.],\n",
            "        [4., 3., 3., 3., 2., 0., 2., 1., 3., 3., 3., 2., 3.],\n",
            "        [4., 3., 3., 3., 2., 2., 0., 1., 3., 3., 3., 2., 3.],\n",
            "        [3., 2., 2., 2., 1., 1., 1., 0., 2., 2., 2., 1., 2.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 0., 2., 2., 1., 4.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 0., 2., 1., 4.],\n",
            "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 2., 0., 1., 4.],\n",
            "        [4., 3., 3., 3., 2., 2., 2., 1., 1., 1., 1., 0., 3.],\n",
            "        [3., 2., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 0.]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "item = corpus[5]\n",
        "tokentree = item.to_tree()\n",
        "ete3_tree = tokentree_to_ete(tokentree)\n",
        "print(ete3_tree, '\\n')\n",
        "\n",
        "gold_distance = create_gold_distances(corpus[5:6])[0]\n",
        "print(gold_distance, '\\n')\n",
        "\n",
        "mst = create_mst(gold_distance)\n",
        "print(mst)\n",
        "print(gold_distance, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ctxxb3LSEwCV"
      },
      "outputs": [],
      "source": [
        "def edges(mst):\n",
        "    # Your code for retrieving the edges from the MST matrix\n",
        "    edges = set()\n",
        "    for i, node1 in enumerate(mst):\n",
        "        for j, node2 in enumerate(node1):\n",
        "            if node2 == 1:\n",
        "                edges.add((i,j))\n",
        "                edges.add((j,i))\n",
        "\n",
        "    return edges\n",
        "\n",
        "def calc_uuas(pred_distances, gold_distances):\n",
        "    uuas = None\n",
        "    # Find the indices of non-padded rows and columns\n",
        "    non_padded_rows = torch.any(gold_distances != -1, dim=1)\n",
        "    non_padded_cols = torch.any(gold_distances != -1, dim=0)\n",
        "\n",
        "    # Slice both tensors using the non-padded indices\n",
        "    pred_distances = pred_distances[non_padded_rows][:, non_padded_cols]\n",
        "    gold_distances = gold_distances[non_padded_rows][:, non_padded_cols]\n",
        "    \n",
        "    pred_mst = create_mst(pred_distances)\n",
        "    gold_mst = create_mst(gold_distances)\n",
        "    pred_edges = edges(pred_mst)\n",
        "    gold_edges = edges(gold_mst)\n",
        "    correct = pred_edges.intersection(gold_edges)\n",
        "    if len(gold_edges) == 0:\n",
        "        uuas = 1\n",
        "    else:\n",
        "        uuas = len(correct) / len(gold_edges)\n",
        "    return uuas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IVfrZG_-EwCV"
      },
      "source": [
        "# Structural probing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dQk736JxEwCV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class StructuralProbe(nn.Module):\n",
        "    \"\"\" Computes squared L2 distance after projection by a matrix.\n",
        "    For a batch of sentences, computes all n^2 pairs of distances\n",
        "    for each sentence in the batch.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_dim, rank, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.probe_rank = rank\n",
        "        self.model_dim = model_dim\n",
        "        \n",
        "        self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
        "        self.proj_norms = nn.Parameter(data=torch.zeros(self.model_dim, self.probe_rank))\n",
        "        \n",
        "        nn.init.uniform_(self.proj, -0.05, 0.05)\n",
        "        nn.init.uniform_(self.proj_norms, -0.05, 0.05)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\" Computes all n^2 pairs of distances after projection\n",
        "        for each sentence in a batch.\n",
        "        Note that due to padding, some distances will be non-zero for pads.\n",
        "        Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
        "        Args:\n",
        "          batch: a batch of word representations of the shape\n",
        "            (batch_size, max_seq_len, representation_dim)\n",
        "        Returns:\n",
        "          A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
        "        \"\"\"\n",
        "        transformed = torch.matmul(batch, self.proj)\n",
        "        transformed_norms = torch.matmul(batch, self.proj_norms)\n",
        "        \n",
        "        batchlen, seqlen, rank = transformed.size()\n",
        "        \n",
        "        transformed = transformed.unsqueeze(2)\n",
        "        transformed = transformed.expand(-1, -1, seqlen, -1)\n",
        "        transposed = transformed.transpose(1,2)\n",
        "        \n",
        "        diffs = transformed - transposed\n",
        "        \n",
        "        squared_diffs = diffs.pow(2)\n",
        "        squared_distances = torch.sum(squared_diffs, -1)\n",
        "        \n",
        "        squared_norms = transformed_norms.pow(2).sum(-1)\n",
        "\n",
        "        return squared_distances, squared_norms\n",
        "\n",
        "    \n",
        "    \n",
        "class L1DistanceLoss(nn.Module):\n",
        "    \"\"\"Custom L1 loss for distance matrices.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predictions, label_batch, length_batch):\n",
        "        \"\"\" Computes L1 loss on distance matrices.\n",
        "        Ignores all entries where label_batch=-1\n",
        "        Normalizes first within sentences (by dividing by the square of the sentence length)\n",
        "        and then across the batch.\n",
        "        Args:\n",
        "          predictions: A pytorch batch of predicted distances\n",
        "          label_batch: A pytorch batch of true distances\n",
        "          length_batch: A pytorch batch of sentence lengths\n",
        "        Returns:\n",
        "          A tuple of:\n",
        "            batch_loss: average loss in the batch\n",
        "            total_sents: number of sentences in the batch\n",
        "        \"\"\"\n",
        "        labels_1s = (label_batch != -1).float()\n",
        "        predictions_masked = predictions * labels_1s\n",
        "        labels_masked = label_batch * labels_1s\n",
        "        total_sents = torch.sum((length_batch != 0)).float()\n",
        "        squared_lengths = length_batch.pow(2).float()\n",
        "\n",
        "        if total_sents > 0:\n",
        "            loss_per_sent = torch.sum(torch.abs(predictions_masked - labels_masked), dim=(1,2))\n",
        "            normalized_loss_per_sent = loss_per_sent / squared_lengths\n",
        "            batch_loss = torch.sum(normalized_loss_per_sent) / total_sents\n",
        "        \n",
        "        else:\n",
        "            batch_loss = torch.tensor(0.0)\n",
        "        \n",
        "        return batch_loss, total_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "6CzYp96BEwCV"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "import math\n",
        "\n",
        "'''\n",
        "Similar to the `create_data` method of the previous notebook, I recommend you to use a method \n",
        "that initialises all the data of a corpus. Note that for your embeddings you can use the \n",
        "`fetch_sen_reps` method again. However, for the POS probe you concatenated all these representations into \n",
        "1 big tensor of shape (num_tokens_in_corpus, model_dim). \n",
        "\n",
        "The StructuralProbe expects its input to contain all the representations of 1 sentence, so I recommend you\n",
        "to update your `fetch_sen_reps` method in a way that it is easy to retrieve all the representations that \n",
        "correspond to a single sentence.\n",
        "''' \n",
        "\n",
        "def init_corpus(path, concat=False, cutoff=None, layer=-1):\n",
        "    \"\"\" Initialises the data of a corpus.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        Path to corpus location\n",
        "    concat : bool, optional\n",
        "        Optional toggle to concatenate all the tensors\n",
        "        returned by `fetch_sen_reps`.\n",
        "    cutoff : int, optional\n",
        "        Optional integer to \"cutoff\" the data in the corpus.\n",
        "        This allows only a subset to be used, alleviating \n",
        "        memory usage.\n",
        "    \"\"\"\n",
        "    corpus = parse_corpus(path)[:cutoff]\n",
        "    embs = fetch_sen_reps(corpus, lstm, vocab, concat=concat, layer=layer)\n",
        "#     embs = fetch_sen_reps(corpus, model, tokenizer, concat=concat, layer=layer)    \n",
        "    gold_distances = create_gold_distances(corpus)\n",
        "    \n",
        "    return gold_distances, embs\n",
        "\n",
        "\n",
        "# I recommend you to write a method that can evaluate the UUAS & loss score for the dev (& test) corpus.\n",
        "# Feel free to alter the signature of this method.\n",
        "def evaluate_probe(probe, _data):\n",
        "    loss_function =  L1DistanceLoss()\n",
        "    probe.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        distances = _data[0]\n",
        "        embs = _data[1]\n",
        "        \n",
        "        # Make big tensor of shape (batch_size, max_seq_len, representation_dim)\n",
        "        # from the representations in the batch using zero-padding\n",
        "        max_seq_len = max([tensor.shape[0] for tensor in embs])\n",
        "        batch_size = len(embs)\n",
        "        representation_dim = embs[0].shape[1]\n",
        "        embs_tensor = torch.zeros(batch_size, max_seq_len, representation_dim)\n",
        "        for i, tensor in enumerate(embs):\n",
        "            if tensor.shape[0] == 1:\n",
        "                tensor = tensor.view(1, -1, representation_dim)[:, 0, :]\n",
        "            seq_len = tensor.shape[0]\n",
        "            embs_tensor[i, :seq_len] = tensor\n",
        "\n",
        "        # Predict distances and norms using probe\n",
        "        predicted_distances, predicted_norms = probe(embs_tensor)\n",
        "\n",
        "        # Make big tensor of distances from batch padded with -1\n",
        "        # resulting in true_distances of shape (batch_size, max_seq_len, max_seq_len)\n",
        "        max_seq_len = max([tensor.shape[0] for tensor in distances])\n",
        "        batch_size = len(distances)\n",
        "        sentence_lengths = torch.zeros(batch_size, dtype=torch.long)\n",
        "        true_distances = torch.full((batch_size, max_seq_len, max_seq_len), -1)\n",
        "        for i, tensor in enumerate(distances):\n",
        "            seq_len = tensor.shape[0]\n",
        "            true_distances[i, :seq_len, :seq_len] = tensor\n",
        "            sentence_lengths[i] = seq_len\n",
        "\n",
        "        # Calculate loss and uass score\n",
        "        loss, total_sents = loss_function(predicted_distances, true_distances, sentence_lengths)\n",
        "        uuas_score = [calc_uuas(pred, true) for pred, true in zip(predicted_distances, true_distances)]\n",
        "    \n",
        "    return loss.item(), uuas_score\n",
        "\n",
        "\n",
        "# Feel free to alter the signature of this method.\n",
        "def train(_train_data, _dev_data, _test_data, device, epochs=1000):\n",
        "    emb_dim = _train_data[1][0].shape[-1]\n",
        "    rank = 64\n",
        "    lr = 10e-4\n",
        "    batch_size = 24\n",
        "    dev_losses = []\n",
        "    dev_uuases = []\n",
        "\n",
        "    probe = StructuralProbe(emb_dim, rank, device)\n",
        "    optimizer = optim.Adam(probe.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,patience=1)\n",
        "    loss_function =  L1DistanceLoss()\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        print(\"Epoch:\", epoch)\n",
        "        for i in range(0, len(_train_data[1]), batch_size):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # YOUR CODE FOR DOING A PROBE FORWARD PASS\n",
        "            batch_embs = _train_data[1][i:i+batch_size]\n",
        "            batch_distances = _train_data[0][i:i+batch_size]\n",
        "            \n",
        "            \n",
        "            # Make big tensor of shape (batch_size, max_seq_len, representation_dim)\n",
        "            # from the representations in the batch using zero-padding\n",
        "            max_seq_len = max([tensor.shape[0] for tensor in batch_embs])\n",
        "            embs_tensor = torch.zeros(len(batch_embs), max_seq_len, emb_dim)\n",
        "            for j, tensor in enumerate(batch_embs):\n",
        "                if tensor.shape[0] == 1:\n",
        "                    tensor = tensor.view(1, -1, emb_dim)[:, 0, :]\n",
        "                seq_len = tensor.shape[0]\n",
        "                embs_tensor[j, :seq_len] = tensor\n",
        "    \n",
        "            # Predict distances and using probe\n",
        "            predicted_distances, predicted_norms = probe(embs_tensor.to(device))\n",
        "            \n",
        "            # Make big tensor of shape (batch_size, max_seq_len, max_seq_len) from distances\n",
        "            # in the batch using padding with -1\n",
        "            max_seq_len = max([tensor.shape[0] for tensor in batch_distances])\n",
        "            sentence_lengths = torch.zeros(len(batch_embs), dtype=torch.long)\n",
        "            true_distances = torch.full((len(batch_embs), max_seq_len, max_seq_len), -1)\n",
        "            for j, tensor in enumerate(batch_distances):\n",
        "                seq_len = tensor.shape[0]\n",
        "                true_distances[j, :seq_len, :seq_len] = tensor\n",
        "                sentence_lengths[j] = seq_len\n",
        "\n",
        "            # Calculate loss\n",
        "            batch_loss, total_sents = loss_function(predicted_distances, true_distances, sentence_lengths)\n",
        "\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        dev_loss, dev_uuas = evaluate_probe(probe, _dev_data)\n",
        "        dev_losses.append(dev_loss)\n",
        "        dev_uuases.append(sum(dev_uuas)/len(dev_uuas))\n",
        "        \n",
        "\n",
        "        # Using a scheduler is up to you, and might require some hyper param fine-tuning\n",
        "        scheduler.step(dev_loss)\n",
        "\n",
        "    test_loss, test_uuas = evaluate_probe(probe, _test_data)\n",
        "    return dev_losses, dev_uuases, test_loss, sum(test_uuas)/len(test_uuas)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qagGv0mdEwCV"
      },
      "source": [
        "# Amnesic probing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "QAzZ-AenEwCW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "def get_probe_projection(probe):\n",
        "    \"\"\"\n",
        "    Function to get the nullspace projection of a linear probe\n",
        "    \"\"\"\n",
        "\n",
        "    null_space_proj = scipy.linalg.null_space(probe)\n",
        "    return null_space_proj\n",
        "\n",
        "\n",
        "def project_representations(data, probe):\n",
        "    \"\"\"\n",
        "    Function to project all training data on the null space of the probe,\n",
        "    as such effectively removing all information conveyed by the probe\n",
        "    \"\"\"\n",
        "    # print(f\"Probe shape: {probe.shape}\")\n",
        "    # print(f\"Data shape: {data.shape}\")\n",
        "\n",
        "    null_space_proj = get_probe_projection(probe)\n",
        "    # print(f\"Null space projection shape: {null_space_proj.shape}\")\n",
        "    # print(f\"Null space projection: {null_space_proj}\")\n",
        "    # print(f\"Vector shape: {first_vec.shape}\")\n",
        "    # print(f\"Projection shape: {proj.shape}\")\n",
        "\n",
        "    projected_data = [torch.from_numpy(np.array(elem)@null_space_proj) for elem in data]\n",
        "\n",
        "    # print(f\"full projection shape: {projected_data.shape}\")\n",
        "    # print(f\"projection: {projected_data}\")\n",
        "    \n",
        "    return projected_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tsr1hAZnEwCW"
      },
      "outputs": [],
      "source": [
        "def get_struct_data(path):\n",
        "\n",
        "    corpus = parse_corpus(path)\n",
        "    gold_distances = create_gold_distances(corpus)\n",
        "    \n",
        "    return gold_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "84jH05hpEwCW",
        "outputId": "1fab5d11-6029-4517-c641-e449f2e0032e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating training distances...\n",
            "Done generating training distances. This took 6.644079208374023s\n",
            "Generating validation distances...\n",
            "Done generating validation distances. This took 2.4767580032348633s\n",
            "Generating testing distances...\n",
            "Done generating testing distances. This took 1.8606925010681152s\n",
            "Generating training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 90/90 [01:43<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done generating training data. This took 104.05411553382874s\n",
            "Generating validation data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done generating validation data. This took 52.390767097473145s\n",
            "Generating test data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:50<00:00,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done generating test data. This took 50.41778302192688s\n",
            "Fitting probe...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done fitting POS probe. This took 0.46842527389526367s\n",
            "Training structural probe...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 1/20 [00:00<00:11,  1.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 2/20 [00:01<00:10,  1.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 3/20 [00:01<00:10,  1.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 4/20 [00:02<00:09,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 5/20 [00:02<00:08,  1.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 6/20 [00:03<00:08,  1.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 7/20 [00:04<00:07,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 8/20 [00:04<00:07,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 9/20 [00:05<00:06,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 10/20 [00:05<00:05,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 11/20 [00:06<00:04,  1.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 12/20 [00:06<00:04,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 13/20 [00:07<00:03,  1.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 14/20 [00:07<00:03,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 15/20 [00:08<00:02,  1.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 16/20 [00:09<00:02,  1.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 17/20 [00:09<00:01,  1.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 18/20 [00:10<00:01,  1.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 19/20 [00:10<00:00,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:11<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done training structural probe. This took 11.430310010910034s\n",
            "Generating training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 64/90 [01:16<00:30,  1.19s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[72], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating training data...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m GPT_train_x, GPT_train_y, GPT_train_vocab \u001b[39m=\u001b[39m create_data(\n\u001b[0;32m     35\u001b[0m     ud_parses,\n\u001b[0;32m     36\u001b[0m     os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mpre_path\u001b[39m}\u001b[39;49;00m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msample\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m use_sample \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39men_ewt-ud-train.conllu\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     37\u001b[0m     model, \n\u001b[0;32m     38\u001b[0m     tokenizer,\n\u001b[0;32m     39\u001b[0m     layer\u001b[39m=\u001b[39;49mlayer\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDone generating training data. This took \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mt\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
            "Cell \u001b[1;32mIn[48], line 9\u001b[0m, in \u001b[0;36mcreate_data\u001b[1;34m(ud_parses, filename, lm, w2i, pos_vocab, layer)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_data\u001b[39m(ud_parses, filename: \u001b[39mstr\u001b[39m, lm, w2i, pos_vocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, layer\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      7\u001b[0m     new_parses \u001b[39m=\u001b[39m parse_corpus(filename)\n\u001b[1;32m----> 9\u001b[0m     sen_reps \u001b[39m=\u001b[39m fetch_sen_reps(new_parses, lm, w2i, concat\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, layer\u001b[39m=\u001b[39;49mlayer)\n\u001b[0;32m     10\u001b[0m     pos_tags, pos_vocab \u001b[39m=\u001b[39m fetch_pos_tags(new_parses, pos_vocab\u001b[39m=\u001b[39mpos_vocab)\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m sen_reps, pos_tags, pos_vocab\n",
            "Cell \u001b[1;32mIn[7], line 47\u001b[0m, in \u001b[0;36mfetch_sen_reps\u001b[1;34m(ud_parses, model, tokenizer, concat, layer)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m# Get the sentence representation from the model's output\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 47\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     49\u001b[0m last_layer_hidden_state \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mhidden_states[layer]\n\u001b[0;32m     50\u001b[0m sen_rep \u001b[39m=\u001b[39m last_layer_hidden_state\u001b[39m.\u001b[39msqueeze()\n",
            "File \u001b[1;32mc:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1098\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   1096\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1098\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head(hidden_states)\n\u001b[0;32m   1100\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[39m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "num_layers = 7\n",
        "epochs = 20\n",
        "test_UUAS_per_layer = []\n",
        "use_sample = True\n",
        "model_name = \"GPT\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# conllu_path = \"/content/drive/My Drive/nlp2-probing-lms/data/\"\n",
        "conllu_path = \"data/\"\n",
        "\n",
        "if use_sample:\n",
        "  conllu_path += \"sample/\"\n",
        "\n",
        "t = time.time()\n",
        "print(f\"Generating training distances...\")\n",
        "gold_distances_train = get_struct_data(f\"{conllu_path}en_ewt-ud-train.conllu\")\n",
        "print(f\"Done generating training distances. This took {time.time() - t}s\")\n",
        "\n",
        "t = time.time()\n",
        "print(f\"Generating validation distances...\")\n",
        "gold_distances_dev = get_struct_data(f\"{conllu_path}en_ewt-ud-dev.conllu\")\n",
        "print(f\"Done generating validation distances. This took {time.time() - t}s\")\n",
        "\n",
        "t = time.time()\n",
        "print(f\"Generating testing distances...\")\n",
        "gold_distances_test = get_struct_data(f\"{conllu_path}en_ewt-ud-test.conllu\")\n",
        "print(f\"Done generating testing distances. This took {time.time() - t}s\")\n",
        "\n",
        "for layer in range(num_layers):\n",
        "\n",
        "    t = time.time()\n",
        "    print(f\"Generating training data...\")\n",
        "    GPT_train_x, GPT_train_y, GPT_train_vocab = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
        "        model, \n",
        "        tokenizer,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating training data. This took {time.time() - t}s\")\n",
        "\n",
        "    t = time.time()\n",
        "    print(f\"Generating validation data...\")\n",
        "    GPT_dev_x, GPT_dev_y, _ = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
        "        model, \n",
        "        tokenizer,\n",
        "        pos_vocab=GPT_train_vocab,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating validation data. This took {time.time() - t}s\")\n",
        "\n",
        "    t = time.time()\n",
        "    print(f\"Generating test data...\")\n",
        "    GPT_test_x, GPT_test_y, _ = create_data(\n",
        "        ud_parses,\n",
        "        os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
        "        model,\n",
        "        tokenizer,\n",
        "        pos_vocab=GPT_train_vocab,\n",
        "        layer=layer\n",
        "    )\n",
        "    print(f\"Done generating test data. This took {time.time() - t}s\")\n",
        "\n",
        "\n",
        "    POS_probe = LogisticRegression()\n",
        "    t = time.time()\n",
        "    print(f\"Fitting probe...\")\n",
        "    POS_probe.fit(torch.cat(GPT_train_x, dim=0), GPT_train_y)\n",
        "    print(f\"Done fitting POS probe. This took {time.time() - t}s\")\n",
        "\n",
        "    probe_weights = POS_probe.coef_\n",
        "\n",
        "    # Get the \"amnesic vectors\", the representations with POS information removed\n",
        "    GPT_train_projected = project_representations(GPT_train_x, probe_weights)\n",
        "    GPT_dev_projected = project_representations(GPT_dev_x, probe_weights)\n",
        "    GPT_test_projected = project_representations(GPT_test_x, probe_weights)\n",
        "\n",
        "    t = time.time()\n",
        "    print(\"Training structural probe...\")\n",
        "    # Train the structural probe on these amnesic vectors\n",
        "    _, _, proj_GPT_test_loss, proj_GPT_test_uuas = train((gold_distances_train, GPT_train_projected),\n",
        "                                                         (gold_distances_dev, GPT_dev_projected),\n",
        "                                                         (gold_distances_test, GPT_test_projected),\n",
        "                                                         device,\n",
        "                                                         epochs)\n",
        "    print(f\"Done training structural probe. This took {time.time() - t}s\")\n",
        "\n",
        "    test_UUAS_per_layer.append(proj_GPT_test_uuas)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Debug cells**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating training distances...\n",
            "Done generating training distances. This took 6.569975137710571s\n",
            "Generating validation distances...\n",
            "Done generating validation distances. This took 2.455244779586792s\n",
            "Generating testing distances...\n",
            "Done generating testing distances. This took 1.8536860942840576s\n",
            "Generating training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 90/90 [01:41<00:00,  1.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done generating training data. This took 101.76648831367493s\n",
            "Generating validation data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:50<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done generating validation data. This took 50.83025908470154s\n",
            "Generating test data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:50<00:00,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done generating test data. This took 51.01414513587952s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import time\n",
        "\n",
        "# num_layers = 7\n",
        "# epochs = 20\n",
        "# test_UUAS_per_layer = []\n",
        "# use_sample = True\n",
        "# model_name = \"GPT\"\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# # conllu_path = \"/content/drive/My Drive/nlp2-probing-lms/data/\"\n",
        "# conllu_path = \"data/\"\n",
        "\n",
        "# if use_sample:\n",
        "#   conllu_path += \"sample/\"\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating training distances...\")\n",
        "# gold_distances_train = get_struct_data(f\"{conllu_path}en_ewt-ud-train.conllu\")\n",
        "# print(f\"Done generating training distances. This took {time.time() - t}s\")\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating validation distances...\")\n",
        "# gold_distances_dev = get_struct_data(f\"{conllu_path}en_ewt-ud-dev.conllu\")\n",
        "# print(f\"Done generating validation distances. This took {time.time() - t}s\")\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating testing distances...\")\n",
        "# gold_distances_test = get_struct_data(f\"{conllu_path}en_ewt-ud-test.conllu\")\n",
        "# print(f\"Done generating testing distances. This took {time.time() - t}s\")\n",
        "\n",
        "# layer = 1\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating training data...\")\n",
        "# GPT_train_x, GPT_train_y, GPT_train_vocab = create_data(\n",
        "#     ud_parses,\n",
        "#     os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
        "#     model, \n",
        "#     tokenizer,\n",
        "#     layer=layer\n",
        "# )\n",
        "# print(f\"Done generating training data. This took {time.time() - t}s\")\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating validation data...\")\n",
        "# GPT_dev_x, GPT_dev_y, _ = create_data(\n",
        "#     ud_parses,\n",
        "#     os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
        "#     model, \n",
        "#     tokenizer,\n",
        "#     pos_vocab=GPT_train_vocab,\n",
        "#     layer=layer\n",
        "# )\n",
        "# print(f\"Done generating validation data. This took {time.time() - t}s\")\n",
        "\n",
        "# t = time.time()\n",
        "# print(f\"Generating test data...\")\n",
        "# GPT_test_x, GPT_test_y, _ = create_data(\n",
        "#     ud_parses,\n",
        "#     os.path.join(f'{pre_path}data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
        "#     model,\n",
        "#     tokenizer,\n",
        "#     pos_vocab=GPT_train_vocab,\n",
        "#     layer=layer\n",
        "# )\n",
        "# print(f\"Done generating test data. This took {time.time() - t}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting probe...\n",
            "768\n",
            "torch.Size([2301, 768])\n",
            "Done fitting POS probe. This took 0.5420012474060059s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# POS_probe = LogisticRegression()\n",
        "# t = time.time()\n",
        "# print(f\"Fitting probe...\")\n",
        "\n",
        "# # print(len(GPT_train_x[2][0]))\n",
        "\n",
        "# # print(torch.cat(GPT_train_x, dim=0).size())\n",
        "# # print(torch.tensor(GPT_train_x).size())\n",
        "\n",
        "# POS_probe.fit(torch.cat(GPT_train_x, dim=0), GPT_train_y)\n",
        "# print(f\"Done fitting POS probe. This took {time.time() - t}s\")\n",
        "\n",
        "# # Predict using the trained model\n",
        "# # GPT_train_predictions = POS_probe.predict(GPT_train_x)\n",
        "# # GPT_dev_predictions = POS_probe.predict(GPT_dev_x)\n",
        "# # GPT_test_predictions = POS_probe.predict(GPT_test_x)\n",
        "\n",
        "# # # Calculate accuracy scores\n",
        "# # GPT_train_accuracy = accuracy_score(GPT_train_y, GPT_train_predictions)\n",
        "# # GPT_dev_accuracy = accuracy_score(GPT_dev_y, GPT_dev_predictions)\n",
        "# # GPT_test_accuracy = accuracy_score(GPT_test_y, GPT_test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training structural probe...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 76, 76])\n",
            "torch.Size([24, 76, 76])\n",
            "torch.Size([24])\n",
            "torch.Size([18, 60, 60])\n",
            "torch.Size([18, 60, 60])\n",
            "torch.Size([18])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 1/20 [00:00<00:11,  1.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 76, 76])\n",
            "torch.Size([24, 76, 76])\n",
            "torch.Size([24])\n",
            "torch.Size([18, 60, 60])\n",
            "torch.Size([18, 60, 60])\n",
            "torch.Size([18])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 2/20 [00:01<00:10,  1.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 76, 76])\n",
            "torch.Size([24, 76, 76])\n",
            "torch.Size([24])\n",
            "torch.Size([18, 60, 60])\n",
            "torch.Size([18, 60, 60])\n",
            "torch.Size([18])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 3/20 [00:01<00:09,  1.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 76, 76])\n",
            "torch.Size([24, 76, 76])\n",
            "torch.Size([24])\n",
            "torch.Size([18, 60, 60])\n",
            "torch.Size([18, 60, 60])\n",
            "torch.Size([18])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 4/20 [00:02<00:10,  1.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24, 40, 40])\n",
            "torch.Size([24])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24, 51, 51])\n",
            "torch.Size([24])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[70], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining structural probe...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Train the structural probe on these amnesic vectors\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m _, _, proj_GPT_test_loss, proj_GPT_test_uuas \u001b[39m=\u001b[39m train((gold_distances_train, GPT_train_projected),\n\u001b[0;32m     16\u001b[0m                                                         (gold_distances_dev, GPT_dev_projected),\n\u001b[0;32m     17\u001b[0m                                                         (gold_distances_test, GPT_test_projected),\n\u001b[0;32m     18\u001b[0m                                                         device,\n\u001b[0;32m     19\u001b[0m                                                         epochs)\n\u001b[0;32m     20\u001b[0m \u001b[39m# print(f\"Done training structural probe. This took {time.time() - t}s\")\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[58], line 116\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(_train_data, _dev_data, _test_data, device, epochs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     embs_tensor[j, :seq_len] \u001b[39m=\u001b[39m tensor\n\u001b[0;32m    115\u001b[0m \u001b[39m# Predict distances and using probe\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m predicted_distances, predicted_norms \u001b[39m=\u001b[39m probe(embs_tensor\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m    118\u001b[0m \u001b[39m# Make big tensor of shape (batch_size, max_seq_len, max_seq_len) from distances\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39m# in the batch using padding with -1\u001b[39;00m\n\u001b[0;32m    120\u001b[0m max_seq_len \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m([tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m batch_distances])\n",
            "File \u001b[1;32mc:\\Users\\Cyproxius\\anaconda3\\envs\\nlp2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[19], line 34\u001b[0m, in \u001b[0;36mStructuralProbe.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Computes all n^2 pairs of distances after projection\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mfor each sentence in a batch.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mNote that due to padding, some distances will be non-zero for pads.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m  A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m transformed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj)\n\u001b[1;32m---> 34\u001b[0m transformed_norms \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj_norms)\n\u001b[0;32m     36\u001b[0m batchlen, seqlen, rank \u001b[39m=\u001b[39m transformed\u001b[39m.\u001b[39msize()\n\u001b[0;32m     38\u001b[0m transformed \u001b[39m=\u001b[39m transformed\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# # print(GPT_train_x.shape())\n",
        "# # print(GPT_train_projected.shape())\n",
        "\n",
        "# probe_weights = POS_probe.coef_\n",
        "\n",
        "# # Get the \"amnesic vectors\", the representations with POS information removed\n",
        "# GPT_train_projected = project_representations(GPT_train_x, probe_weights)\n",
        "# GPT_dev_projected = project_representations(GPT_dev_x, probe_weights)\n",
        "# GPT_test_projected = project_representations(GPT_test_x, probe_weights)\n",
        "\n",
        "# t = time.time()\n",
        "# print(\"Training structural probe...\")\n",
        "\n",
        "# # Train the structural probe on these amnesic vectors\n",
        "# _, _, proj_GPT_test_loss, proj_GPT_test_uuas = train((gold_distances_train, GPT_train_projected),\n",
        "#                                                         (gold_distances_dev, GPT_dev_projected),\n",
        "#                                                         (gold_distances_test, GPT_test_projected),\n",
        "#                                                         device,\n",
        "#                                                         epochs)\n",
        "# print(f\"Done training structural probe. This took {time.time() - t}s\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0054a95d49324d98965135558d69f4af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6835cea3a77456684bc55c7ba1ee83d",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6674badd6934fe6a5ffd5b71e8e5500",
            "value": 124
          }
        },
        "095d6aa08abe442a867cd3149465c85c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0efc75fc035a481db06dffb4c67ffe5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9684e6be3e11412f956bfb042da9b586",
            "max": 762,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9c9f81639244160a5d6b2b03ae7af10",
            "value": 762
          }
        },
        "12d249faa23d4210adcdd33ef19c0d68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12e3aa25538c4be49e659a473b06996a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61a8246918c242ebaed044594ddc2909",
            "placeholder": "​",
            "style": "IPY_MODEL_b155fb2de4d548c5bf603eea303195a5",
            "value": " 353M/353M [00:08&lt;00:00, 37.7MB/s]"
          }
        },
        "16b26704213c4bedaeea101cd4055423": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a7c07d906cb495193c2563cb74f0b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c2fbe4f99414ce08a014e3edac25f66",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c28397a9e410436099f9e854f3953f26",
            "value": 456318
          }
        },
        "1c2fbe4f99414ce08a014e3edac25f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c6998ba327b406d81d4fde7c53084c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "239d8ce4aefb4b8b9607bed1f551932b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0018b1d2645410781e306b8b5e0ba85",
              "IPY_MODEL_0054a95d49324d98965135558d69f4af",
              "IPY_MODEL_97fa77a9a3d246bfb2869d853941ee0c"
            ],
            "layout": "IPY_MODEL_eba3f378e34e4cc2b18a85a86878b2f8"
          }
        },
        "26e3c86bc6424d9d82b57e98de567412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d3927d68fde43e8b1a1714dcbfc6d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3412d6e0a822403e9eced42a27be5e83",
            "placeholder": "​",
            "style": "IPY_MODEL_b93fb7c0f94f4e87b10a9f1f4cce1f72",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "2d6d617f980a4be890f4530579e394f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3412d6e0a822403e9eced42a27be5e83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "411cbda37a5b4b97a296523d5aee68ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41706e356ea54429ae9bed2ea07cb985": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ba945dd773f403da285123f401bd956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50a87e2d410a4646aac69e6ec569fe45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce49711278fc4c2d8e3bbcd0080b4ee8",
              "IPY_MODEL_0efc75fc035a481db06dffb4c67ffe5c",
              "IPY_MODEL_79ef487a2c634720a5b5e224873f2124"
            ],
            "layout": "IPY_MODEL_63965dde784144f3b26f207702df4c5f"
          }
        },
        "53c6e7fe69c14742a5d9e5dddaca6ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d3927d68fde43e8b1a1714dcbfc6d70",
              "IPY_MODEL_b5203c4c023644e5af4d2fb583ed8ab7",
              "IPY_MODEL_609fe980bf6e4a0193e8b8762b97e2ea"
            ],
            "layout": "IPY_MODEL_89a0aceec46445fcac15cad80c25143a"
          }
        },
        "573333e7d39e4ac1abe6268ab58a9eef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "609fe980bf6e4a0193e8b8762b97e2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcf097747c2049fb8405bc2b73a72a78",
            "placeholder": "​",
            "style": "IPY_MODEL_095d6aa08abe442a867cd3149465c85c",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 6.53MB/s]"
          }
        },
        "60f47f8819a34b2eb59eecc6e030d9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ddb3327a9f94b5998d7e090683c57d6",
              "IPY_MODEL_6f5fcfcc935444acb984c8efd1bb079c",
              "IPY_MODEL_12e3aa25538c4be49e659a473b06996a"
            ],
            "layout": "IPY_MODEL_b5434fd6ed81433dbbea72201f2a2278"
          }
        },
        "61a8246918c242ebaed044594ddc2909": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63965dde784144f3b26f207702df4c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "678e392ed6954fe2b8fc2c5f18697c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70cc5fde5a9a4564936d1678630b31e5",
            "placeholder": "​",
            "style": "IPY_MODEL_1c6998ba327b406d81d4fde7c53084c1",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "68e4a1838fc44b1fb3332372957bb3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f5fcfcc935444acb984c8efd1bb079c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbd7bac5d4e748818521de315047aaa0",
            "max": 352833716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ba945dd773f403da285123f401bd956",
            "value": 352833716
          }
        },
        "70cc5fde5a9a4564936d1678630b31e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ef487a2c634720a5b5e224873f2124": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4f270d070b94337857d4dac32dc0917",
            "placeholder": "​",
            "style": "IPY_MODEL_dee330099c9943399078d7e7079c501a",
            "value": " 762/762 [00:00&lt;00:00, 60.6kB/s]"
          }
        },
        "89a0aceec46445fcac15cad80c25143a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ddb3327a9f94b5998d7e090683c57d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0dbf077edd145708b5eccd1cb27ac6c",
            "placeholder": "​",
            "style": "IPY_MODEL_a7f1e0b0fc9c47da9fd0cdade0ee270e",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "9684e6be3e11412f956bfb042da9b586": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97fa77a9a3d246bfb2869d853941ee0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d6d617f980a4be890f4530579e394f3",
            "placeholder": "​",
            "style": "IPY_MODEL_68e4a1838fc44b1fb3332372957bb3b4",
            "value": " 124/124 [00:00&lt;00:00, 8.66kB/s]"
          }
        },
        "9e5a221a72024d1f9241d7dadff8a9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6674badd6934fe6a5ffd5b71e8e5500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7f1e0b0fc9c47da9fd0cdade0ee270e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b155fb2de4d548c5bf603eea303195a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b46fc293aee74cdbbcbc05659a463e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_678e392ed6954fe2b8fc2c5f18697c0a",
              "IPY_MODEL_1a7c07d906cb495193c2563cb74f0b93",
              "IPY_MODEL_c74447bd75894fd99e02d51fd3251910"
            ],
            "layout": "IPY_MODEL_573333e7d39e4ac1abe6268ab58a9eef"
          }
        },
        "b5203c4c023644e5af4d2fb583ed8ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6a194c45c754640820dc951dac1aabe",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3625843bf7c47bbb9851a72358063fb",
            "value": 1042301
          }
        },
        "b5434fd6ed81433dbbea72201f2a2278": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b93fb7c0f94f4e87b10a9f1f4cce1f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcf097747c2049fb8405bc2b73a72a78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28397a9e410436099f9e854f3953f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6a194c45c754640820dc951dac1aabe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74447bd75894fd99e02d51fd3251910": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_411cbda37a5b4b97a296523d5aee68ee",
            "placeholder": "​",
            "style": "IPY_MODEL_41706e356ea54429ae9bed2ea07cb985",
            "value": " 456k/456k [00:00&lt;00:00, 8.12MB/s]"
          }
        },
        "ce49711278fc4c2d8e3bbcd0080b4ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12d249faa23d4210adcdd33ef19c0d68",
            "placeholder": "​",
            "style": "IPY_MODEL_26e3c86bc6424d9d82b57e98de567412",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "d0018b1d2645410781e306b8b5e0ba85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b26704213c4bedaeea101cd4055423",
            "placeholder": "​",
            "style": "IPY_MODEL_9e5a221a72024d1f9241d7dadff8a9a6",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "d0dbf077edd145708b5eccd1cb27ac6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f270d070b94337857d4dac32dc0917": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6835cea3a77456684bc55c7ba1ee83d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd7bac5d4e748818521de315047aaa0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee330099c9943399078d7e7079c501a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9c9f81639244160a5d6b2b03ae7af10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eba3f378e34e4cc2b18a85a86878b2f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3625843bf7c47bbb9851a72358063fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
